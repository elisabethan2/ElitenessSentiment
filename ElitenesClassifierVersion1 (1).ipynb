{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Colab, Google Cloud bucket. Change project ID"
      ],
      "metadata": {
        "id": "AFw-ggfWLLEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUZaPV41LDiQ",
        "outputId": "930df9a7-ad0d-4942-d709-834ad108b8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#colab code: mount to drive to import and export data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install libaries and connect to Google Cloud account  to import and export data\n",
        "!pip install google-cloud-storage transformers torch tqdm pandas\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set your project ID\n",
        "!gcloud config set project tokyo-silicon-441818-f7  # Replace with your actual project ID"
      ],
      "metadata": {
        "id": "q1Z3vwGsLTH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install libaries"
      ],
      "metadata": {
        "id": "AE8h3srPLZ0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download da_core_news_lg #Download the Danish language model for spacy"
      ],
      "metadata": {
        "id": "wamE-XCaLXaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading and test/train split.\n",
        "Data is lemmatized. Only run on 1 file because of time constraints.\n",
        "Change input and output paths if necessary."
      ],
      "metadata": {
        "id": "eRlSUkT0MPsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#byt evt. lemmatized ud med ikke lemmatized og tokenize i stedet for\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "class NewsDataPreparator:\n",
        "    def __init__(self):\n",
        "        self.file_paths = [\n",
        "            #\"/content/drive/MyDrive/NewsData/Lemmatized_2007_1M.txt\",\n",
        "            #\"/content/drive/MyDrive/NewsData/Lemmatized_2008_300K.txt\",\n",
        "            #\"/content/drive/MyDrive/NewsData/Lemmatized_2011_1M.txt\",\n",
        "            #\"/content/drive/MyDrive/NewsData/Lemmatized_2015_1M.txt\",\n",
        "            #\"/content/drive/MyDrive/NewsData/Lemmatized_2016_1M.txt\",\n",
        "            #\"/content/drive/MyDrive/NewsData/Lemmatized_2017_1M.txt\",\n",
        "            #\"/content/drive/MyDrive/NewsData/Lemmatized_2019_1M.txt\",\n",
        "            #\"/content/drive/MyDrive/NewsData/Lemmatized_2020_1M.txt\",\n",
        "            \"/content/drive/MyDrive/NewsData/Lemmatized_2021_1M.txt\"\n",
        "        ]\n",
        "\n",
        "    def verify_file_access(self):\n",
        "        \"\"\"Verify access to all files and return accessible files\"\"\"\n",
        "        accessible_files = []\n",
        "        print(\"Verifying file access...\")\n",
        "        for file_path in self.file_paths:\n",
        "            if os.path.exists(file_path):\n",
        "                accessible_files.append(file_path)\n",
        "                print(f\"✓ Found: {os.path.basename(file_path)}\")\n",
        "            else:\n",
        "                print(f\"✗ Not found: {os.path.basename(file_path)}\")\n",
        "\n",
        "        if not accessible_files:\n",
        "            raise FileNotFoundError(\"No input files are accessible. Please check file paths and permissions.\")\n",
        "\n",
        "        return accessible_files\n",
        "\n",
        "    def load_single_file(self, file_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Load a single file and convert to DataFrame\"\"\"\n",
        "        try:\n",
        "            # Extract year from filename\n",
        "            year = file_path.split('_')[1]\n",
        "\n",
        "            # Read the file line by line\n",
        "            data = []\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                for line_num, line in enumerate(file, 1):\n",
        "                    line = line.strip()\n",
        "                    if line:  # Only process non-empty lines\n",
        "                        data.append({\n",
        "                            'year': year,\n",
        "                            'sentence_id': line_num,\n",
        "                            'sentence': line,\n",
        "                            'file_source': os.path.basename(file_path)\n",
        "                        })\n",
        "\n",
        "                    if line_num % 100000 == 0:  # Progress update for large files\n",
        "                        print(f\"Processed {line_num} lines from {os.path.basename(file_path)}\")\n",
        "\n",
        "            if not data:\n",
        "                print(f\"Warning: No valid data extracted from {file_path}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            df = pd.DataFrame(data)\n",
        "            print(f\"Successfully loaded {len(df)} sentences from {os.path.basename(file_path)}\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {file_path}: {str(e)}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def load_all_files(self) -> pd.DataFrame:\n",
        "        \"\"\"Load all accessible files and combine into single DataFrame\"\"\"\n",
        "        print(\"\\nLoading files...\")\n",
        "        all_data = []\n",
        "\n",
        "        # First verify which files are accessible\n",
        "        accessible_files = self.verify_file_access()\n",
        "\n",
        "        # Load each accessible file\n",
        "        for file_path in tqdm(accessible_files):\n",
        "            df = self.load_single_file(file_path)\n",
        "            if not df.empty:\n",
        "                all_data.append(df)\n",
        "\n",
        "        if not all_data:\n",
        "            raise ValueError(\"No data was successfully loaded from any files\")\n",
        "\n",
        "        # Combine all DataFrames\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        print(f\"\\nTotal loaded: {len(combined_df)} sentences from {len(all_data)} files\")\n",
        "\n",
        "        # Basic data quality check\n",
        "        print(\"\\nSentences per year:\")\n",
        "        print(combined_df['year'].value_counts().sort_index())\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def create_train_val_split(self, df: pd.DataFrame, val_size: float = 0.2) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Split data into training and validation sets\"\"\"\n",
        "        train_df, val_df = train_test_split(\n",
        "            df,\n",
        "            test_size=val_size,\n",
        "            random_state=42,\n",
        "            stratify=df['year']\n",
        "        )\n",
        "\n",
        "        print(f\"Training set: {len(train_df)} sentences\")\n",
        "        print(f\"Validation set: {len(val_df)} sentences\")\n",
        "\n",
        "        return {\n",
        "            'train': train_df,\n",
        "            'validation': val_df\n",
        "        }\n",
        "\n",
        "    def save_processed_data(self, data_dict: Dict[str, pd.DataFrame], output_dir: str):\n",
        "        \"\"\"Save processed datasets\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        for name, df in data_dict.items():\n",
        "            output_path = os.path.join(output_dir, f'{name}_data.csv')\n",
        "            df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "            print(f\"Saved {name} data to {output_path}\")\n",
        "            print(f\"Sample from {name} data:\")\n",
        "            print(df.head(2))\n",
        "\n",
        "def main():\n",
        "    # Initialize data preparator\n",
        "    preparator = NewsDataPreparator()\n",
        "\n",
        "    # Load all files\n",
        "    print(\"Step 1: Loading and parsing files...\")\n",
        "    all_data = preparator.load_all_files()\n",
        "\n",
        "    # Create train/validation split\n",
        "    print(\"\\nStep 2: Creating train/validation split...\")\n",
        "    data_splits = preparator.create_train_val_split(all_data)\n",
        "\n",
        "    # Save processed data\n",
        "    print(\"\\nStep 3: Saving processed data...\")\n",
        "    output_dir = \"/content/drive/MyDrive/NewsData/processed\"\n",
        "    preparator.save_processed_data(data_splits, output_dir)\n",
        "\n",
        "    return data_splits\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    processed_data = main()"
      ],
      "metadata": {
        "id": "KFQRt5z-MOyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pattern matching in batches seperately for train and validation set. Same script for train and validation set. Change input and output paths."
      ],
      "metadata": {
        "id": "Rn51TRmHXy_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from typing import Dict, List, Set, Tuple\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "class DanishPatternStructure:\n",
        "    def __init__(self):\n",
        "        self.market_categories = {\n",
        "            'energy_and_green_transition': {\n",
        "                'titles': [\n",
        "                    'klimadirektør', 'klima direktør', 'klimachef', 'klima chef',\n",
        "                    'bæredygtighedschef', 'bæredygtighed chef', 'miljøchef', 'miljø chef',\n",
        "                    'energidirektør', 'energi direktør', 'vindmølle', 'kraft værk',\n",
        "                    'energi chef', 'miljø direktør', 'bæredygtighed direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'energiselskab', 'energi selskab', 'vindmølle', 'miljø',\n",
        "                    'forsyning', 'energistyrelse', 'klima', 'miljøstyrelse'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'grøn', 'bæredygtig', 'vedvarende', 'energi', 'klima',\n",
        "                    'miljø', 'vindkraft', 'power', 'omstilling'\n",
        "                ]\n",
        "            },\n",
        "            'welfare': {\n",
        "                'titles': [\n",
        "                    'velfærdsdirektør', 'velfærd direktør', 'socialchef', 'social chef',\n",
        "                    'ældrechef', 'ældre chef', 'børnechef', 'børne chef', 'ungechef',\n",
        "                    'beskæftigelseschef', 'beskæftigelse chef', 'integrationschef'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'kommune', 'region', 'socialstyrelse', 'beskæftigelse',\n",
        "                    'socialministerium', 'velfærd', 'ældrecenter', 'børnehus'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'velfærd', 'social', 'ældre', 'børn', 'unge',\n",
        "                    'beskæftigelse', 'integration', 'kommune', 'omsorg'\n",
        "                ]\n",
        "            },\n",
        "            'maritime_and_shipping': {\n",
        "                'titles': [\n",
        "                    'rederdirektør', 'reder', 'havnedirektør', 'havn chef',\n",
        "                    'skibsreder', 'marinechef', 'marine chef', 'offshore chef',\n",
        "                    'logistikchef', 'logistik direktør', 'fragtchef'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'rederi', 'havn', 'værft', 'søfart', 'maritime',\n",
        "                    'offshore', 'shipping', 'container', 'fragt'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'søfart', 'shipping', 'maritim', 'offshore', 'havn',\n",
        "                    'skib', 'container', 'logistik', 'fragt'\n",
        "                ]\n",
        "            },\n",
        "            'agriculture_food': {\n",
        "                'titles': [\n",
        "                    'landbrugsdirektør', 'landbrug chef', 'fødevarechef', 'fødevare direktør',\n",
        "                    'mejerichef', 'mejeri direktør', 'landbrugspræsident', 'fødevaredirektør',\n",
        "                    'landmand', 'gårdejer'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'landbrug', 'fødevare', 'mejeri', 'slagteri',\n",
        "                    'landbrugsorganisation', 'fødevarestyrelse', 'gård'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'landbrug', 'fødevare', 'mejeri', 'økologi',\n",
        "                    'fødevaresikkerhed', 'eksport', 'gård', 'mark'\n",
        "                ]\n",
        "            },\n",
        "            'union_labour': {\n",
        "                'titles': [\n",
        "                    'forbundsformand', 'forbund formand', 'fagforeningsformand',\n",
        "                    'hovedkasserer', 'forhandlingsleder', 'forhandling chef',\n",
        "                    'arbejdsmarkedschef', 'arbejdsmarked direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'fagforening', 'forbund', 'hovedorganisation', 'akasse',\n",
        "                    'arbejdsgiver', 'overenskomst', 'fagbevægelse'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'overenskomst', 'arbejdsmarked', 'fagbevægelse', 'medlem',\n",
        "                    'forhandling', 'arbejdsret', 'faglig'\n",
        "                ]\n",
        "            },\n",
        "            'real_estate': {\n",
        "                'titles': [\n",
        "                    'ejendomsmægler', 'developer', 'ejendomsadministrator',\n",
        "                    'bygherrerådgiver', 'ejendom chef', 'bolig direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'ejendomsselskab', 'developer', 'boligforening', 'administration',\n",
        "                    'boligselskab', 'ejendomsadministration'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'ejendom', 'bolig', 'byggeri', 'udlejning', 'investering',\n",
        "                    'developer', 'administration', 'bygherre'\n",
        "                ]\n",
        "            },\n",
        "            'finance': {\n",
        "                'titles': [\n",
        "                    'bankdirektør', 'bank chef', 'finansdirektør', 'finans chef',\n",
        "                    'investor', 'analytiker', 'økonom', 'fondforvalter',\n",
        "                    'porteføljemanager', 'aktiestrateg', 'valutahandler'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'bank', 'børs', 'investering', 'kapitalfond', 'nationalbank',\n",
        "                    'realkredit', 'pension', 'forsikring'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'finans', 'aktie', 'obligation', 'investering', 'rente',\n",
        "                    'marked', 'valuta', 'børs', 'opkøb', 'fusion'\n",
        "                ]\n",
        "            },\n",
        "            'industry': {\n",
        "                'titles': [\n",
        "                    'administrerende direktør', 'adm direktør', 'bestyrelsesformand',\n",
        "                    'koncernchef', 'koncern direktør', 'fabriksdirektør',\n",
        "                    'produktionschef', 'industri chef'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'virksomhed', 'koncern', 'industri', 'produktion',\n",
        "                    'erhverv', 'fabrik', 'industri'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'produktion', 'industri', 'marked', 'fabrik', 'supply chain',\n",
        "                    'erhverv', 'koncern', 'virksomhed'\n",
        "                ]\n",
        "            },\n",
        "            'tech': {\n",
        "                'titles': [\n",
        "                    'udviklingschef', 'udvikling direktør', 'techekspert', 'tech chef',\n",
        "                    'itdirektør', 'it chef', 'softwareudvikler', 'dataanalytiker',\n",
        "                    'digital chef', 'innovation direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'tech', 'startup', 'software', 'ai', 'teknologi',\n",
        "                    'it', 'digital', 'data'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'teknologi', 'digital', 'innovation', 'data', 'kunstig intelligens',\n",
        "                    'automatisering', 'software', 'it', 'tech'\n",
        "                ]\n",
        "            },\n",
        "            'regulatory': {\n",
        "                'titles': [\n",
        "                    'minister', 'direktør', 'formand', 'tilsynschef',\n",
        "                    'departementchef', 'styrelsesdirektør', 'afdelingschef'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'ministerium', 'styrelse', 'tilsyn', 'myndighed',\n",
        "                    'domstol', 'departement', 'forvaltning'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'regulering', 'lovgivning', 'tilsyn', 'politik',\n",
        "                    'beskatning', 'myndighed', 'forvaltning'\n",
        "                ]\n",
        "            },\n",
        "            'education': {\n",
        "                'titles': [\n",
        "                    'rektor', 'professor', 'lektor', 'underviser',\n",
        "                    'uddannelseschef', 'uddannelse direktør', 'skoleleder',\n",
        "                    'dekan', 'institutleder'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'universitet', 'gymnasium', 'skole', 'uddannelse',\n",
        "                    'institut', 'fakultet', 'akademi'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'uddannelse', 'forskning', 'læring', 'pædagogik',\n",
        "                    'student', 'elev', 'undervisning'\n",
        "                ]\n",
        "            },\n",
        "            'healthcare': {\n",
        "                'titles': [\n",
        "                    'læge', 'sygeplejerske', 'hospitaldirektør', 'hospital chef',\n",
        "                    'specialist', 'sundhedsøkonom', 'forskningschef', 'overlæge',\n",
        "                    'produktchef', 'regulatory affairs', 'medicinal direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'hospital', 'klinik', 'sundhedsstyrelse', 'apotek',\n",
        "                    'medicinal', 'biotek', 'sundhed', 'læge'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'sundhed', 'patient', 'medicin', 'behandling', 'sygdom',\n",
        "                    'diabetes', 'biotek', 'klinisk', 'insulin'\n",
        "                ]\n",
        "            },\n",
        "            'politics': {\n",
        "                'titles': [\n",
        "                    'politiker', 'minister', 'borgmester', 'rådmand',\n",
        "                    'mfer', 'folketingsmedlem', 'regionsrådsformand',\n",
        "                    'kommunalbestyrelsesmedlem'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'folketing', 'byråd', 'ministerium', 'parti',\n",
        "                    'kommission', 'regering', 'kommunalbestyrelse'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'politik', 'valg', 'beslutning', 'samfund',\n",
        "                    'demokrati', 'lovgivning', 'reform'\n",
        "                ]\n",
        "            },\n",
        "            'aviation': {\n",
        "                'titles': [\n",
        "                    'pilot', 'flyveleder', 'luftfartsdirektør', 'luftfart chef',\n",
        "                    'kabinechef', 'teknisk chef', 'lufthavnsdirektør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'luftfart', 'lufthavn', 'flyproducent', 'flyselskab',\n",
        "                    'aviation', 'airline'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'fly', 'luftfart', 'rejse', 'sikkerhed', 'transport',\n",
        "                    'lufthavn', 'aviation', 'airline'\n",
        "                ]\n",
        "            },\n",
        "            'design': {\n",
        "                'titles': [\n",
        "                    'designer', 'kreativ direktør', 'produktudvikler',\n",
        "                    'modeekspert', 'designchef', 'art director'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'designstudie', 'modehus', 'designfirma',\n",
        "                    'tegnestue', 'kreativ', 'mode'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'design', 'mode', 'produkt', 'æstetik',\n",
        "                    'bruger', 'kreativ', 'kunst'\n",
        "                ]\n",
        "            },\n",
        "            'architecture': {\n",
        "                'titles': [\n",
        "                    'arkitekt', 'bygningsdesigner', 'landskabsarkitekt',\n",
        "                    'byplanlægger', 'partner', 'kreativ direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'arkitektfirma', 'tegnestue', 'byplanlægning',\n",
        "                    'arkitektur', 'design'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'arkitektur', 'byrum', 'byggeri', 'design',\n",
        "                    'æstetik', 'byplanlægning', 'landskab'\n",
        "                ]\n",
        "            },\n",
        "            'hospitality': {\n",
        "                'titles': [\n",
        "                    'hotelchef', 'hotel direktør', 'kok', 'restauratør',\n",
        "                    'sommelier', 'restaurantchef', 'køkkenchef'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'hotel', 'restaurant', 'catering', 'gastronomi',\n",
        "                    'hospitality'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'hotel', 'restaurant', 'mad', 'overnatning',\n",
        "                    'oplevelse', 'service', 'gastronomi'\n",
        "                ]\n",
        "            },\n",
        "            'tourism': {\n",
        "                'titles': [\n",
        "                    'turistchef', 'turist direktør', 'rejseleder',\n",
        "                    'destinationschef', 'destination direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'turistkontor', 'rejsebureau', 'destination',\n",
        "                    'turisme', 'rejse'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'rejse', 'turisme', 'oplevelse', 'destination',\n",
        "                    'attraktion', 'ferie', 'turist'\n",
        "                ]\n",
        "            },\n",
        "            'appliances': {\n",
        "                'titles': [\n",
        "                    'produktchef', 'produkt direktør', 'ingeniør',\n",
        "                    'udviklingschef', 'teknisk chef', 'salgschef'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'hvidevare', 'elektronik', 'distribution',\n",
        "                    'producent', 'forhandler'\n",
        "                ],\n",
        "                'keywords': [\n",
        "                    'hvidevare', 'elektronik', 'køkken', 'innovation',\n",
        "                    'produkt', 'teknisk', 'udvikling'\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.international_markers = {\n",
        "            'eu': {\n",
        "                'titles': [\n",
        "                    'EU-kommissær', 'EU kommissær', 'EUkommissær',\n",
        "                    'MEP', 'europaparlamentariker', 'europa parlamentariker',\n",
        "                    'EU-direktør', 'EU direktør', 'EUdirektør',\n",
        "                    'EU-chef', 'EU chef', 'EUchef',\n",
        "                    'europa chef', 'europæisk direktør', 'europa direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'EU-Kommission', 'EU Kommission', 'EUKommission',\n",
        "                    'Europaparlament', 'Europa Parlament', 'Europa-Parlament',\n",
        "                    'EU-Domstol', 'EU Domstol', 'EUDomstol',\n",
        "                    'EU-agentur', 'EU agentur', 'EUagentur',\n",
        "                    'EU-kontor', 'EU kontor', 'EUkontor',\n",
        "                    'europæisk institution', 'europa institution'\n",
        "                ]\n",
        "            },\n",
        "            'nordic': {\n",
        "                'titles': [\n",
        "                    'nordisk direktør', 'nordisk chef', 'nordisk leder',\n",
        "                    'skandinavienchef', 'skandinavien chef', 'skandinavisk direktør',\n",
        "                    'nordisk koordinator', 'skandinavisk leder', 'nordic director',\n",
        "                    'nordisk ansvarlig', 'skandinavisk ansvarlig'\n",
        "              ],\n",
        "                'orgs': [\n",
        "                    'Nordisk Råd', 'Nordisk Raad', 'Nordisk Ministerråd',\n",
        "                    'Nordisk Ministerraad', 'skandinavisk afdeling',\n",
        "                    'skandinavisk kontor', 'nordisk afdeling', 'nordisk kontor',\n",
        "                    'nordic office', 'skandinavisk division'\n",
        "                ]\n",
        "            },\n",
        "            'regional_danish': {\n",
        "                'titles': [\n",
        "                    'regionsdirektør', 'regions direktør', 'region direktør',\n",
        "                    'borgmester', 'kommunaldirektør', 'kommunal direktør',\n",
        "                    'regionsrådsformand', 'regionsraad formand', 'regionsråd formand',\n",
        "                    'kommunal chef', 'regional chef', 'regions chef',\n",
        "                    'regional direktør', 'lokal direktør'\n",
        "                ],\n",
        "                'orgs': [\n",
        "                    'region', 'kommune', 'lokalråd', 'lokal råd',\n",
        "                    'byråd', 'byraad', 'kommunal råd', 'kommunalråd',\n",
        "                    'regional administration', 'kommunal forvaltning',\n",
        "                    'regional myndighed', 'kommunal myndighed'\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.elite_hierarchy = {\n",
        "            'top_level': {\n",
        "                'titles': [\n",
        "                    'koncernchef', 'koncern chef', 'konsernchef',\n",
        "                    'administrerende direktør', 'adm direktør', 'adm dir',\n",
        "                    'administrerende dir', 'adm. direktør', 'topchef',\n",
        "                    'top chef', 'bestyrelsesformand', 'bestyrelses formand',\n",
        "                    'president', 'CEO', 'chief executive officer',\n",
        "                    'grundlægger', 'grundlæger', 'stifter',\n",
        "                    'ejer', 'direktionsformand', 'direktions formand',\n",
        "                    'group ceo', 'koncerndirektør', 'koncern direktør'\n",
        "                ],\n",
        "                'indicators': [\n",
        "                    'koncern', 'gruppe', 'holding', 'international',\n",
        "                    'group', 'worldwide', 'global', 'nordic',\n",
        "                    'skandinavisk', 'europæisk', 'executive'\n",
        "                ]\n",
        "            },\n",
        "            'senior_level': {\n",
        "                'titles': [\n",
        "                    'direktør', 'områdechef', 'område chef',\n",
        "                    'afdelingschef', 'afdelings chef', 'afdeling chef',\n",
        "                    'regionsdirektør', 'regions direktør', 'region direktør',\n",
        "                    'landechef', 'lande chef', 'land chef',\n",
        "                    'divisionsdirektør', 'divisions direktør', 'division direktør',\n",
        "                    'partner', 'senior manager', 'senior direktør'\n",
        "                ],\n",
        "                'indicators': [\n",
        "                    'senior', 'chef', 'leder', 'manager',\n",
        "                    'director', 'head', 'ansvarlig', 'lead'\n",
        "                ]\n",
        "            },\n",
        "            'expert_level': {\n",
        "                'titles': [\n",
        "                    'chefanalytiker', 'chef analytiker', 'specialkonsulent',\n",
        "                    'special konsulent', 'seniorøkonom', 'senior økonom',\n",
        "                    'chefforsker', 'chef forsker', 'ekspert',\n",
        "                    'specialist', 'seniorrådgiver', 'senior rådgiver',\n",
        "                    'chief analyst', 'senior specialist', 'senior advisor'\n",
        "                ],\n",
        "                'indicators': [\n",
        "                    'specialist', 'ekspert', 'forsker', 'analyst',\n",
        "                    'researcher', 'advisor', 'consultant', 'expert',\n",
        "                    'analytiker', 'rådgiver', 'konsulent'\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "        self.org_forms = {\n",
        "            'private': [\n",
        "                'A/S', 'AS', 'ApS', 'APS', 'I/S', 'IS',\n",
        "                'K/S', 'KS', 'P/S', 'PS', 'IVS',\n",
        "                'Holding', 'Group', 'Gruppen', 'Koncern',\n",
        "                'Koncernen', 'Danmark', 'Danish', 'International',\n",
        "                'Global', 'Nordic', 'Skandinavisk'\n",
        "            ],\n",
        "            # Additional organization forms omitted for brevity\n",
        "            'public': [\n",
        "                'Kommune', 'Kommunen', 'Region', 'Regionen',\n",
        "                'Ministerium', 'Ministeriet', 'Styrelse',\n",
        "                'Styrelsen', 'Direktorat', 'Direktoratet',\n",
        "                'Institut', 'Instituttet', 'Center', 'Centret',\n",
        "                'Forvaltning', 'Forvaltningen', 'Myndighed',\n",
        "                'Myndigheden', 'Råd', 'Rådet'\n",
        "            ],\n",
        "            'associations': [\n",
        "                'Forening', 'Foreningen', 'Forbund', 'Forbundet',\n",
        "                'Organisation', 'Organisationen', 'Fond', 'Fonden',\n",
        "                'Fagforening', 'Fagforeningen', 'Sammenslutning',\n",
        "                'Sammenslutningen', 'Selskab', 'Selskabet',\n",
        "                'Forening', 'NGO', 'Interesseorganisation'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Add lemmatized forms of compound connectors\n",
        "        self.compound_connectors = {\n",
        "            's': ['erhverv', 'forbund', 'regering', 'arbejd', 'uddannelse', 'udvikling'],\n",
        "            'e': ['børn', 'folk', 'kommun', 'skol', 'virksomhed'],\n",
        "            'r': ['lær', 'led', 'arbejd', 'direktør', 'chef']\n",
        "        }\n",
        "\n",
        "        # Add lemmatized forms of prefixes and suffixes\n",
        "        self.common_prefixes = [\n",
        "            'over', 'under', 'mellem', 'chef', 'top', 'vice', 'først', 'senior', 'junior',\n",
        "            'special', 'hoved', 'general', 'central', 'koncern', 'gruppe', 'region'\n",
        "        ]\n",
        "\n",
        "        self.common_suffixes = [\n",
        "            'chef', 'direktør', 'leder', 'ansvarlig', 'koordinator', 'konsulent',\n",
        "            'specialist', 'analytiker', 'rådgiver', 'formand', 'præsident'\n",
        "        ]\n",
        "\n",
        "\n",
        "    def get_all_patterns(self, term: str) -> List[str]:\n",
        "        \"\"\"Generate all possible patterns for a term including lemmatized forms\"\"\"\n",
        "        patterns = [term]\n",
        "\n",
        "        # Add space-separated version\n",
        "        if '-' in term:\n",
        "            patterns.append(term.replace('-', ' '))\n",
        "\n",
        "        # Add compound variations\n",
        "        for connector in self.compound_connectors.keys():\n",
        "            patterns.append(f\"{term}{connector}\")\n",
        "\n",
        "        # Add common prefix/suffix combinations\n",
        "        for prefix in self.common_prefixes:\n",
        "            patterns.append(f\"{prefix}{term}\")\n",
        "            patterns.append(f\"{prefix}-{term}\")\n",
        "            patterns.append(f\"{prefix} {term}\")\n",
        "\n",
        "        for suffix in self.common_suffixes:\n",
        "            patterns.append(f\"{term}{suffix}\")\n",
        "            patterns.append(f\"{term}-{suffix}\")\n",
        "            patterns.append(f\"{term} {suffix}\")\n",
        "\n",
        "        return patterns\n",
        "class PatternMatchingAnalyzer:\n",
        "    def __init__(self, batch_size=50000):\n",
        "        self.batch_size = batch_size\n",
        "        self.danish_patterns = DanishPatternStructure()\n",
        "        self.compiled_patterns = self._precompile_patterns()\n",
        "\n",
        "    def _precompile_patterns(self):\n",
        "        \"\"\"Precompile all patterns once during initialization\"\"\"\n",
        "        patterns = {}\n",
        "        for category, type_dict in self.danish_patterns.market_categories.items():\n",
        "            patterns[category] = {}\n",
        "            for type_name, terms in type_dict.items():\n",
        "                patterns[category][type_name] = self._compile_patterns_for_terms(terms)\n",
        "\n",
        "        # Add patterns for other dictionaries if needed\n",
        "        for scope, data in self.danish_patterns.international_markers.items():\n",
        "            patterns[f'international_{scope}'] = {\n",
        "                type_name: self._compile_patterns_for_terms(terms)\n",
        "                for type_name, terms in data.items()\n",
        "            }\n",
        "\n",
        "        # Add hierarchy patterns\n",
        "        for level, data in self.danish_patterns.elite_hierarchy.items():\n",
        "            patterns[f'hierarchy_{level}'] = {\n",
        "                type_name: self._compile_patterns_for_terms(terms)\n",
        "                for type_name, terms in data.items()\n",
        "            }\n",
        "\n",
        "        return patterns\n",
        "\n",
        "    def _compile_patterns_for_terms(self, terms):\n",
        "        \"\"\"Compile regex patterns for a list of terms\"\"\"\n",
        "        return [re.compile(fr'\\b{re.escape(term)}\\b', re.IGNORECASE | re.UNICODE)\n",
        "                for term in terms]\n",
        "\n",
        "    def _find_matches_in_text(self, text: str) -> Dict[str, Dict[str, Set[str]]]:\n",
        "        \"\"\"Find all pattern matches in text\"\"\"\n",
        "        results = defaultdict(lambda: defaultdict(set))\n",
        "\n",
        "        for category, type_patterns in self.compiled_patterns.items():\n",
        "            for type_name, patterns in type_patterns.items():\n",
        "                for pattern in patterns:\n",
        "                    matches = pattern.findall(text)\n",
        "                    if matches:\n",
        "                        results[category][type_name].update(matches)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def process_batch(self, sentences: List[str], batch_num: int) -> Dict:\n",
        "        \"\"\"Process a batch of sentences\"\"\"\n",
        "        batch_results = defaultdict(lambda: defaultdict(set))\n",
        "\n",
        "        for sentence in tqdm(sentences, desc=f\"Processing batch {batch_num}\"):\n",
        "            matches = self._find_matches_in_text(sentence)\n",
        "            for category, type_dict in matches.items():\n",
        "                for type_name, found_matches in type_dict.items():\n",
        "                    batch_results[category][type_name].update(found_matches)\n",
        "\n",
        "        return {\n",
        "            'markers': {k: {sk: list(sv) for sk, sv in v.items()}\n",
        "                      for k, v in batch_results.items()}\n",
        "        }\n",
        "\n",
        "    def save_batch_results(self, batch_results: Dict, batch_num: int, output_dir: str):\n",
        "        \"\"\"Save batch results with timestamp\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        batch_path = os.path.join(output_dir, f\"batch_{batch_num}_{timestamp}.json\")\n",
        "        with open(batch_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(batch_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Saved batch {batch_num} results\")\n",
        "\n",
        "def main():\n",
        "    # Initialize analyzer\n",
        "    analyzer = PatternMatchingAnalyzer(batch_size=50000)\n",
        "\n",
        "    # Set up directories\n",
        "    input_path = \"/content/drive/MyDrive/NewsData/processed/train_data.csv\" #skift mellem train og validation\n",
        "    output_dir = \"/content/drive/MyDrive/NewsData/processed/Elitenessbatchestrain101224\" #ændr navn v. validation\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        print(f\"Loading data from: {input_path}\")\n",
        "        df = pd.read_csv(input_path, usecols=['sentence'])\n",
        "\n",
        "        # Process first batch and validate\n",
        "        print(\"\\nProcessing first batch for validation...\")\n",
        "        first_batch_sentences = df['sentence'].iloc[:analyzer.batch_size].tolist()\n",
        "        first_batch_results = analyzer.process_batch(first_batch_sentences, 0)\n",
        "\n",
        "        # Validate first batch\n",
        "        validation_results = validate_first_batch(first_batch_results)\n",
        "        print_validation_results(validation_results)\n",
        "\n",
        "        # Ask for user confirmation\n",
        "        if validation_results['status'] == 'invalid':\n",
        "            print(\"\\nWarning: Validation found issues with the data structure.\")\n",
        "\n",
        "        user_input = input(\"\\nDo you want to continue processing all batches? (yes/no): \")\n",
        "\n",
        "        if user_input.lower() != 'yes':\n",
        "            print(\"Stopping after first batch\")\n",
        "            return\n",
        "\n",
        "        # Save first batch if continuing\n",
        "        analyzer.save_batch_results(first_batch_results, 0, output_dir)\n",
        "\n",
        "        # Process remaining batches\n",
        "        total_sentences = len(df)\n",
        "        total_batches = (total_sentences + analyzer.batch_size - 1) // analyzer.batch_size\n",
        "\n",
        "        print(f\"\\nProcessing remaining {total_batches-1} batches...\")\n",
        "\n",
        "        for batch_num in range(1, total_batches):\n",
        "            start_idx = batch_num * analyzer.batch_size\n",
        "            end_idx = min(start_idx + analyzer.batch_size, total_sentences)\n",
        "\n",
        "            batch_sentences = df['sentence'].iloc[start_idx:end_idx].tolist()\n",
        "\n",
        "            try:\n",
        "                batch_results = analyzer.process_batch(batch_sentences, batch_num)\n",
        "                analyzer.save_batch_results(batch_results, batch_num, output_dir)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(\"\\nProcessing complete!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "a63adoxnXzNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge and save results.\n",
        "Same script for train and validation set. Change input and output paths"
      ],
      "metadata": {
        "id": "wjFPoWMRNFti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#merging and saving results\n",
        "import json\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Set\n",
        "import glob\n",
        "\n",
        "class BatchMerger:\n",
        "    def __init__(self, input_dir: str):\n",
        "        self.input_dir = input_dir\n",
        "\n",
        "    def get_batch_files(self) -> List[str]:\n",
        "        \"\"\"Get all batch result files, excluding summary files\"\"\"\n",
        "        files = glob.glob(os.path.join(self.input_dir, \"batch_*.json\"))\n",
        "        return [f for f in files if not f.endswith('_summary.json')]\n",
        "\n",
        "    def merge_batches(self) -> Dict:\n",
        "        \"\"\"Merge all batch files into a single dictionary\"\"\"\n",
        "        merged_results = defaultdict(lambda: defaultdict(set))\n",
        "        processed_files = 0\n",
        "\n",
        "        batch_files = self.get_batch_files()\n",
        "        print(f\"Found {len(batch_files)} batch files to merge\")\n",
        "\n",
        "        for file_path in sorted(batch_files):\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    batch_data = json.load(f)\n",
        "\n",
        "                # Merge markers from this batch\n",
        "                if 'markers' in batch_data:\n",
        "                    for category, type_dict in batch_data['markers'].items():\n",
        "                        for type_name, matches in type_dict.items():\n",
        "                            # Convert matches to set to remove duplicates\n",
        "                            merged_results[category][type_name].update(matches)\n",
        "\n",
        "                processed_files += 1\n",
        "                print(f\"Processed file {processed_files}/{len(batch_files)}: {os.path.basename(file_path)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Convert sets to lists for JSON serialization\n",
        "        final_results = {\n",
        "            category: {\n",
        "                type_name: sorted(list(matches))\n",
        "                for type_name, matches in type_dict.items()\n",
        "            }\n",
        "            for category, type_dict in merged_results.items()\n",
        "        }\n",
        "\n",
        "        return final_results\n",
        "\n",
        "    def generate_statistics(self, merged_results: Dict) -> Dict:\n",
        "        \"\"\"Generate statistics about the merged results\"\"\"\n",
        "        stats = {\n",
        "            'total_categories': len(merged_results),\n",
        "            'categories': {},\n",
        "            'total_matches': 0\n",
        "        }\n",
        "\n",
        "        for category, type_dict in merged_results.items():\n",
        "            category_stats = {\n",
        "                'total_matches': 0,\n",
        "                'types': {}\n",
        "            }\n",
        "\n",
        "            for type_name, matches in type_dict.items():\n",
        "                num_matches = len(matches)\n",
        "                category_stats['types'][type_name] = num_matches\n",
        "                category_stats['total_matches'] += num_matches\n",
        "                stats['total_matches'] += num_matches\n",
        "\n",
        "            stats['categories'][category] = category_stats\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def save_merged_results(self, merged_results: Dict, stats: Dict):\n",
        "        \"\"\"Save merged results and statistics\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Save merged results\n",
        "        results_filename = f\"merged_results_{timestamp}.json\"\n",
        "        results_path = os.path.join(self.input_dir, results_filename)\n",
        "\n",
        "        with open(results_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(merged_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Save statistics\n",
        "        stats_filename = f\"merged_results_stats_{timestamp}.json\"\n",
        "        stats_path = os.path.join(self.input_dir, stats_filename)\n",
        "\n",
        "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(stats, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\nResults saved to: {results_filename}\")\n",
        "        print(f\"Statistics saved to: {stats_filename}\")\n",
        "\n",
        "    # Print summary statistics\n",
        "        print(\"\\nSummary Statistics:\")\n",
        "        print(f\"Total main categories: {stats['total_main_categories']}\")\n",
        "        print(f\"Total matches: {stats['total_matches']}\")\n",
        "        print(\"\\nMatches by main category:\")\n",
        "        for main_category, main_cat_stats in stats['main_categories'].items():\n",
        "            print(f\"\\n{main_category}:\")\n",
        "            print(f\"  Total matches: {main_cat_stats['total_matches']}\")\n",
        "            for subcategory, subcat_stats in main_cat_stats['subcategories'].items():\n",
        "                print(f\"\\n  {subcategory}:\")\n",
        "                print(f\"    Total matches: {subcat_stats['total_matches']}\")\n",
        "                for type_name, count in subcat_stats['types'].items():\n",
        "                    print(f\"    - {type_name}: {count}\")\n",
        "\n",
        "def main():\n",
        "    # Directory containing the batch files\n",
        "    input_dir = \"/content/drive/MyDrive/NewsData/processed/ElitenessbatchesValidation101224\" #skift mellem train og validation\n",
        "\n",
        "    try:\n",
        "        # Initialize merger\n",
        "        merger = BatchMerger(input_dir)\n",
        "\n",
        "        # Merge batches\n",
        "        print(\"Starting merge process...\")\n",
        "        merged_results = merger.merge_batches()\n",
        "\n",
        "        # Generate statistics\n",
        "        print(\"\\nGenerating statistics...\")\n",
        "        stats = merger.generate_statistics(merged_results)\n",
        "\n",
        "        # Save results and statistics\n",
        "        print(\"\\nSaving results...\")\n",
        "        merger.save_merged_results(merged_results, stats)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during merge process: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "n7bcRRmFNF4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content check"
      ],
      "metadata": {
        "id": "tvGz6Ce8ST1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check kategorierne og indholdet af dem\n",
        "\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from pprint import pprint\n",
        "\n",
        "def analyze_category_structure(file_path):\n",
        "    \"\"\"\n",
        "    Analyze the category structure of the merged results JSON file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Analyze structure\n",
        "        category_stats = defaultdict(lambda: defaultdict(int))\n",
        "        category_examples = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "        # Collect statistics and examples\n",
        "        for main_category, subcats in data.items():\n",
        "            for subcat, items in subcats.items():\n",
        "                # Count items\n",
        "                category_stats[main_category][subcat] = len(items)\n",
        "\n",
        "                # Store first few examples\n",
        "                category_examples[main_category][subcat] = items[:3]\n",
        "\n",
        "        # Print analysis\n",
        "        print(\"\\nCategory Structure Analysis:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for main_cat in sorted(category_stats.keys()):\n",
        "            print(f\"\\nMain Category: {main_cat}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            for subcat, count in sorted(category_stats[main_cat].items()):\n",
        "                print(f\"\\nSubcategory: {subcat}\")\n",
        "                print(f\"Count: {count}\")\n",
        "                print(\"Examples:\", ', '.join(category_examples[main_cat][subcat]))\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(\"\\nSummary Statistics:\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Total main categories: {len(category_stats)}\")\n",
        "        total_items = sum(sum(subcats.values()) for subcats in category_stats.values())\n",
        "        print(f\"Total items across all categories: {total_items}\")\n",
        "\n",
        "        return category_stats, category_examples\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing file: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# Run analysis\n",
        "file_path = \"/content/drive/MyDrive/NewsData/processed/Elitenessbatches091224/merged_results_20241210_110522.json\"\n",
        "stats, examples = analyze_category_structure(file_path)"
      ],
      "metadata": {
        "id": "gHXHR_BSSTeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First validation check. Comparison between train and validation set"
      ],
      "metadata": {
        "id": "RimSe-LTSfl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from typing import Dict, List, Set, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "class PatternValidator:\n",
        "    def __init__(self, predictions_path: str, ground_truth_path: str):\n",
        "        self.predictions = self._load_json(predictions_path)\n",
        "        self.ground_truth = self._load_json(ground_truth_path)\n",
        "        self.main_categories = ['market_categories', 'elite_hierarchy', 'org_forms', 'international_markers']\n",
        "        self.subcategories = [\n",
        "            'energy_and_green_transition', 'welfare', 'maritime_and_shipping', 'agriculture_food',\n",
        "            'union_labour', 'real_estate', 'finance', 'industry', 'tech', 'regulatory',\n",
        "            'education', 'healthcare', 'politics', 'aviation', 'design', 'architecture',\n",
        "            'hospitality', 'tourism', 'appliances'\n",
        "        ]\n",
        "\n",
        "    def _load_json(self, path: str) -> Dict:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def calculate_category_distribution(self) -> Dict:\n",
        "        \"\"\"Calculate distribution of matches across categories\"\"\"\n",
        "        distribution = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        for dataset in [self.ground_truth, self.predictions]:\n",
        "            for main_cat in self.main_categories:\n",
        "                if main_cat in dataset:\n",
        "                    for subcat in dataset[main_cat]:\n",
        "                        if isinstance(dataset[main_cat][subcat], dict):\n",
        "                            for type_name in ['titles', 'orgs', 'keywords']:\n",
        "                                if type_name in dataset[main_cat][subcat]:\n",
        "                                    distribution[main_cat][subcat] += len(dataset[main_cat][subcat][type_name])\n",
        "\n",
        "        return distribution\n",
        "\n",
        "    def calculate_metrics(self) -> Dict:\n",
        "        \"\"\"Calculate precision, recall, and F1 score for each category and type\"\"\"\n",
        "        metrics = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
        "\n",
        "        for main_cat in self.main_categories:\n",
        "            if main_cat not in self.ground_truth or main_cat not in self.predictions:\n",
        "                continue\n",
        "\n",
        "            for subcat in self.subcategories:\n",
        "                if subcat not in self.ground_truth[main_cat] or subcat not in self.predictions[main_cat]:\n",
        "                    continue\n",
        "\n",
        "                for type_name in ['titles', 'orgs', 'keywords']:\n",
        "                    if (type_name not in self.ground_truth[main_cat][subcat] or\n",
        "                        type_name not in self.predictions[main_cat][subcat]):\n",
        "                        continue\n",
        "\n",
        "                    true_set = set(self.ground_truth[main_cat][subcat][type_name])\n",
        "                    pred_set = set(self.predictions[main_cat][subcat][type_name])\n",
        "\n",
        "                    tp = len(true_set & pred_set)\n",
        "                    fp = len(pred_set - true_set)\n",
        "                    fn = len(true_set - pred_set)\n",
        "\n",
        "                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "                    metrics[main_cat][subcat][type_name] = {\n",
        "                        'precision': precision,\n",
        "                        'recall': recall,\n",
        "                        'f1': f1,\n",
        "                        'true_positives': tp,\n",
        "                        'false_positives': fp,\n",
        "                        'false_negatives': fn,\n",
        "                        'examples': {\n",
        "                            'correct_matches': list(true_set & pred_set)[:5],\n",
        "                            'false_positives': list(pred_set - true_set)[:5],\n",
        "                            'missed_matches': list(true_set - pred_set)[:5]\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def get_error_analysis(self) -> Dict:\n",
        "        \"\"\"Analyze patterns in false positives and false negatives\"\"\"\n",
        "        error_analysis = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
        "\n",
        "        for main_cat in self.main_categories:\n",
        "            if main_cat not in self.ground_truth or main_cat not in self.predictions:\n",
        "                continue\n",
        "\n",
        "            for subcat in self.subcategories:\n",
        "                if subcat not in self.ground_truth[main_cat] or subcat not in self.predictions[main_cat]:\n",
        "                    continue\n",
        "\n",
        "                for type_name in ['titles', 'orgs', 'keywords']:\n",
        "                    if (type_name not in self.ground_truth[main_cat][subcat] or\n",
        "                        type_name not in self.predictions[main_cat][subcat]):\n",
        "                        continue\n",
        "\n",
        "                    true_set = set(self.ground_truth[main_cat][subcat][type_name])\n",
        "                    pred_set = set(self.predictions[main_cat][subcat][type_name])\n",
        "\n",
        "                    # Analyze false positives\n",
        "                    for fp in (pred_set - true_set):\n",
        "                        error_analysis[main_cat][subcat][type_name]['false_positives'].append({\n",
        "                            'term': fp,\n",
        "                            'similar_to': self._find_similar_terms(fp, true_set)\n",
        "                        })\n",
        "\n",
        "                    # Analyze false negatives\n",
        "                    for fn in (true_set - pred_set):\n",
        "                        error_analysis[main_cat][subcat][type_name]['false_negatives'].append({\n",
        "                            'term': fn,\n",
        "                            'similar_to': self._find_similar_terms(fn, pred_set)\n",
        "                        })\n",
        "\n",
        "        return error_analysis\n",
        "\n",
        "    def _find_similar_terms(self, term: str, term_set: Set[str], threshold: float = 0.8) -> List[str]:\n",
        "        \"\"\"Find similar terms using string similarity\"\"\"\n",
        "        from difflib import SequenceMatcher\n",
        "\n",
        "        similar_terms = []\n",
        "        for other_term in term_set:\n",
        "            similarity = SequenceMatcher(None, term.lower(), other_term.lower()).ratio()\n",
        "            if similarity >= threshold:\n",
        "                similar_terms.append((other_term, similarity))\n",
        "\n",
        "        return [term for term, _ in sorted(similar_terms, key=lambda x: x[1], reverse=True)[:3]]\n",
        "\n",
        "    def save_validation_results(self, output_path: str):\n",
        "        \"\"\"Save validation results to JSON\"\"\"\n",
        "        # Calculate distribution, metrics and error analysis\n",
        "        distribution = self.calculate_category_distribution()\n",
        "        metrics = self.calculate_metrics()\n",
        "        error_analysis = self.get_error_analysis()\n",
        "\n",
        "        # Calculate overall metrics for each main category and subcategory\n",
        "        overall_metrics = defaultdict(lambda: defaultdict(dict))\n",
        "        for main_cat in metrics:\n",
        "            for subcat in metrics[main_cat]:\n",
        "                metrics_list = [\n",
        "                    metrics[main_cat][subcat][t]\n",
        "                    for t in metrics[main_cat][subcat]\n",
        "                ]\n",
        "\n",
        "                overall_metrics[main_cat][subcat] = {\n",
        "                    'precision': np.mean([m['precision'] for m in metrics_list]),\n",
        "                    'recall': np.mean([m['recall'] for m in metrics_list]),\n",
        "                    'f1': np.mean([m['f1'] for m in metrics_list])\n",
        "                }\n",
        "\n",
        "        # Prepare validation report\n",
        "        validation_report = {\n",
        "            'distribution': distribution,\n",
        "            'metrics': metrics,\n",
        "            'error_analysis': error_analysis,\n",
        "            'overall_metrics': overall_metrics,\n",
        "            'summary': {\n",
        "                'overall_precision': np.mean([\n",
        "                    m[t]['precision']\n",
        "                    for m in metrics.values()\n",
        "                    for s in m.values()\n",
        "                    for t in s.keys()\n",
        "                ]),\n",
        "                'overall_recall': np.mean([\n",
        "                    m[t]['recall']\n",
        "                    for m in metrics.values()\n",
        "                    for s in m.values()\n",
        "                    for t in s.keys()\n",
        "                ]),\n",
        "                'overall_f1': np.mean([\n",
        "                    m[t]['f1']\n",
        "                    for m in metrics.values()\n",
        "                    for s in m.values()\n",
        "                    for t in s.keys()\n",
        "                ])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save to file\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(validation_report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nValidation Summary:\")\n",
        "        print(f\"Overall Precision: {validation_report['summary']['overall_precision']:.3f}\")\n",
        "        print(f\"Overall Recall: {validation_report['summary']['overall_recall']:.3f}\")\n",
        "        print(f\"Overall F1 Score: {validation_report['summary']['overall_f1']:.3f}\")\n",
        "\n",
        "        # Print category-wise summary\n",
        "        print(\"\\nCategory-wise Summary:\")\n",
        "        for main_cat in overall_metrics:\n",
        "            print(f\"\\n{main_cat}:\")\n",
        "            for subcat in overall_metrics[main_cat]:\n",
        "                metrics = overall_metrics[main_cat][subcat]\n",
        "                print(f\"  {subcat}:\")\n",
        "                print(f\"    Precision: {metrics['precision']:.3f}\")\n",
        "                print(f\"    Recall: {metrics['recall']:.3f}\")\n",
        "                print(f\"    F1: {metrics['f1']:.3f}\")\n",
        "\n",
        "def main():\n",
        "    # Specific paths to your files\n",
        "    validation_results_path = \"/content/drive/MyDrive/NewsData/processed/ElitenessBatchesValidation/merged_results_20241216_181011.json\"\n",
        "    training_results_path = \"/content/drive/MyDrive/NewsData/processed/ElitenessBatchesTrain/merged_results_20241215_203840.json\"\n",
        "\n",
        "    # Path for saving validation analysis\n",
        "    output_path = \"/content/drive/MyDrive/NewsData/processed/validation_analysis_2024161224.json\"\n",
        "\n",
        "    print(\"Loading validation and training results...\")\n",
        "    print(f\"Validation results path: {validation_results_path}\")\n",
        "    print(f\"Training results path: {training_results_path}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize validator using training results as predictions and validation as ground truth\n",
        "        validator = PatternValidator(\n",
        "            predictions_path=training_results_path,\n",
        "            ground_truth_path=validation_results_path\n",
        "        )\n",
        "\n",
        "        # Run validation and save results\n",
        "        print(\"\\nCalculating validation metrics...\")\n",
        "        validator.save_validation_results(output_path)\n",
        "\n",
        "        print(f\"\\nValidation analysis saved to: {output_path}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: Could not find one of the input files: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during validation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09DNStVQ668o",
        "outputId": "99638c36-b170-47fc-f31d-559d3f9a5461"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading validation and training results...\n",
            "Validation results path: /content/drive/MyDrive/NewsData/processed/ElitenessBatchesValidation/merged_results_20241216_181011.json\n",
            "Training results path: /content/drive/MyDrive/NewsData/processed/ElitenessBatchesTrain/merged_results_20241215_203840.json\n",
            "\n",
            "Calculating validation metrics...\n",
            "Error during validation: dictionary changed size during iteration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Første check af sammenligning mellem train og validation ift. dictionaries genereret i train. Ikke overraskende ikke så stort overlap. En del markeret som falsk positive (men er det nok ikke)\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from typing import Dict, List, Set, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "class PatternValidator:\n",
        "    def __init__(self, predictions_path: str, ground_truth_path: str):\n",
        "        self.predictions = self._load_json(predictions_path)\n",
        "        self.ground_truth = self._load_json(ground_truth_path)\n",
        "\n",
        "    def _load_json(self, path: str) -> Dict:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def calculate_metrics(self) -> Dict:\n",
        "        \"\"\"Calculate precision, recall, and F1 score for each category\"\"\"\n",
        "        metrics = defaultdict(dict)\n",
        "\n",
        "        for category in self.ground_truth.keys():\n",
        "            if category not in self.predictions:\n",
        "                print(f\"Warning: Category {category} not found in predictions\")\n",
        "                continue\n",
        "\n",
        "            for type_name in ['titles', 'orgs', 'keywords']:\n",
        "                if type_name not in self.ground_truth[category]:\n",
        "                    continue\n",
        "\n",
        "                # Get ground truth and predicted sets\n",
        "                true_set = set(self.ground_truth[category][type_name])\n",
        "                pred_set = set(self.predictions[category][type_name])\n",
        "\n",
        "                # Calculate metrics\n",
        "                tp = len(true_set & pred_set)  # True positives\n",
        "                fp = len(pred_set - true_set)  # False positives\n",
        "                fn = len(true_set - pred_set)  # False negatives\n",
        "\n",
        "                # Calculate precision, recall, F1\n",
        "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "                metrics[category][type_name] = {\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'true_positives': tp,\n",
        "                    'false_positives': fp,\n",
        "                    'false_negatives': fn,\n",
        "                    'examples': {\n",
        "                        'correct_matches': list(true_set & pred_set)[:5],\n",
        "                        'false_positives': list(pred_set - true_set)[:5],\n",
        "                        'missed_matches': list(true_set - pred_set)[:5]\n",
        "                    }\n",
        "                }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def get_error_analysis(self) -> Dict:\n",
        "        \"\"\"Analyze patterns in false positives and false negatives\"\"\"\n",
        "        error_analysis = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
        "\n",
        "        for category in self.ground_truth.keys():\n",
        "            if category not in self.predictions:\n",
        "                continue\n",
        "\n",
        "            for type_name in ['titles', 'orgs', 'keywords']:\n",
        "                if type_name not in self.ground_truth[category]:\n",
        "                    continue\n",
        "\n",
        "                true_set = set(self.ground_truth[category][type_name])\n",
        "                pred_set = set(self.predictions[category][type_name])\n",
        "\n",
        "                # Analyze false positives\n",
        "                for fp in (pred_set - true_set):\n",
        "                    error_analysis[category][type_name]['false_positives'].append({\n",
        "                        'term': fp,\n",
        "                        'similar_to': self._find_similar_terms(fp, true_set)\n",
        "                    })\n",
        "\n",
        "                # Analyze false negatives\n",
        "                for fn in (true_set - pred_set):\n",
        "                    error_analysis[category][type_name]['false_negatives'].append({\n",
        "                        'term': fn,\n",
        "                        'similar_to': self._find_similar_terms(fn, pred_set)\n",
        "                    })\n",
        "\n",
        "        return error_analysis\n",
        "\n",
        "    def _find_similar_terms(self, term: str, term_set: Set[str], threshold: float = 0.8) -> List[str]:\n",
        "        \"\"\"Find similar terms using string similarity\"\"\"\n",
        "        from difflib import SequenceMatcher\n",
        "\n",
        "        similar_terms = []\n",
        "        for other_term in term_set:\n",
        "            similarity = SequenceMatcher(None, term.lower(), other_term.lower()).ratio()\n",
        "            if similarity >= threshold:\n",
        "                similar_terms.append((other_term, similarity))\n",
        "\n",
        "        return [term for term, _ in sorted(similar_terms, key=lambda x: x[1], reverse=True)[:3]]\n",
        "\n",
        "    def save_validation_results(self, output_path: str):\n",
        "        \"\"\"Save validation results to JSON\"\"\"\n",
        "        # Calculate metrics and error analysis\n",
        "        metrics = self.calculate_metrics()\n",
        "        error_analysis = self.get_error_analysis()\n",
        "\n",
        "        # Prepare validation report\n",
        "        validation_report = {\n",
        "            'metrics': metrics,\n",
        "            'error_analysis': error_analysis,\n",
        "            'summary': {\n",
        "                'overall_precision': np.mean([\n",
        "                    m[t]['precision']\n",
        "                    for m in metrics.values()\n",
        "                    for t in m.keys()\n",
        "                ]),\n",
        "                'overall_recall': np.mean([\n",
        "                    m[t]['recall']\n",
        "                    for m in metrics.values()\n",
        "                    for t in m.keys()\n",
        "                ]),\n",
        "                'overall_f1': np.mean([\n",
        "                    m[t]['f1']\n",
        "                    for m in metrics.values()\n",
        "                    for t in m.keys()\n",
        "                ])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Save to file\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(validation_report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\nValidation Summary:\")\n",
        "        print(f\"Overall Precision: {validation_report['summary']['overall_precision']:.3f}\")\n",
        "        print(f\"Overall Recall: {validation_report['summary']['overall_recall']:.3f}\")\n",
        "        print(f\"Overall F1 Score: {validation_report['summary']['overall_f1']:.3f}\")\n",
        "\n",
        "def main():\n",
        "    # Specific paths to your files\n",
        "    validation_results_path = \"/content/drive/MyDrive/NewsData/processed/ElitenessbatchesValidation101224/merged_resultsValidation_20241210_124802.json\"\n",
        "    training_results_path = \"/content/drive/MyDrive/NewsData/processed/Elitenessbatches091224/merged_results_20241210_110522.json\"\n",
        "\n",
        "    # Path for saving validation analysis\n",
        "    output_path = \"/content/drive/MyDrive/NewsData/processed/validation_analysis_20241210.json\"\n",
        "\n",
        "    print(\"Loading validation and training results...\")\n",
        "    print(f\"Validation results path: {validation_results_path}\")\n",
        "    print(f\"Training results path: {training_results_path}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize validator using training results as predictions and validation as ground truth\n",
        "        validator = PatternValidator(\n",
        "            predictions_path=training_results_path,  # Training results are our predictions\n",
        "            ground_truth_path=validation_results_path  # Validation results are our ground truth\n",
        "        )\n",
        "\n",
        "        # Run validation and save results\n",
        "        print(\"\\nCalculating validation metrics...\")\n",
        "        validator.save_validation_results(output_path)\n",
        "\n",
        "        print(f\"\\nValidation analysis saved to: {output_path}\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: Could not find one of the input files: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during validation: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ggyMG2FvSey5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings creation for all markers using BERT for Danish"
      ],
      "metadata": {
        "id": "v48LkBjqSspX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding etc. creates embeddings and similarity files.\n",
        "\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Initialize BERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/danish-bert-botxo\")\n",
        "model = AutoModel.from_pretrained(\"Maltehb/danish-bert-botxo\")\n",
        "\n",
        "def load_categories():\n",
        "    \"\"\"Load eliteness categories from GCS\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(\"eliteness\")\n",
        "    blob = bucket.blob(\"merged_results_20241210_110522.json\")\n",
        "    categories = json.loads(blob.download_as_string())\n",
        "    print(f\"Loaded categories from GCS\")\n",
        "    return categories\n",
        "\n",
        "def get_word_embedding(word):\n",
        "    \"\"\"Get embedding for a single word/pointer\"\"\"\n",
        "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # Get the actual word embedding, not special tokens\n",
        "        word_embedding = outputs.last_hidden_state[0][1].numpy()\n",
        "    return word_embedding\n",
        "\n",
        "def process_subcategory_batch(category_name, subcat, items, batch_id, sub_batch_size=1000):\n",
        "    \"\"\"Process a subcategory in smaller sub-batches\"\"\"\n",
        "    sub_batch_embeddings = {}\n",
        "    sub_batch_map = {}\n",
        "\n",
        "    for i in range(0, len(items), sub_batch_size):\n",
        "        sub_batch = items[i:i + sub_batch_size]\n",
        "        print(f\"\\nProcessing {category_name}_{subcat} sub-batch {i//sub_batch_size}\")\n",
        "\n",
        "        for item in tqdm(sub_batch):\n",
        "            try:\n",
        "                embedding = get_word_embedding(item)\n",
        "                sub_batch_embeddings[item] = embedding\n",
        "                sub_batch_map[item] = f\"{category_name}_{subcat}\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {item}: {str(e)}\")\n",
        "\n",
        "        # Save sub-batch\n",
        "        if sub_batch_embeddings:\n",
        "            save_batch_results(\n",
        "                sub_batch_embeddings,\n",
        "                sub_batch_map,\n",
        "                f\"{batch_id}_{subcat}_{i//sub_batch_size}\",\n",
        "                category_name\n",
        "            )\n",
        "\n",
        "    return sub_batch_embeddings, sub_batch_map\n",
        "\n",
        "def process_category_batch(category_name, subcategories, batch_id):\n",
        "    \"\"\"Process each subcategory separately\"\"\"\n",
        "    batch_embeddings = {}\n",
        "    batch_map = {}\n",
        "\n",
        "    print(f\"\\nProcessing category: {category_name} (Batch {batch_id})\")\n",
        "\n",
        "    for subcat, items in subcategories.items():\n",
        "        if isinstance(items, list):\n",
        "            print(f\"\\nProcessing subcategory: {subcat} ({len(items)} items)\")\n",
        "            sub_embeddings, sub_map = process_subcategory_batch(\n",
        "                category_name, subcat, items, batch_id\n",
        "            )\n",
        "            batch_embeddings.update(sub_embeddings)\n",
        "            batch_map.update(sub_map)\n",
        "\n",
        "            # Clear memory\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "    # Save complete category results\n",
        "    if batch_embeddings:\n",
        "        save_batch_results(batch_embeddings, batch_map, batch_id, category_name)\n",
        "\n",
        "    return batch_embeddings, batch_map\n",
        "\n",
        "def save_batch_results(batch_embeddings, batch_map, batch_id, category_name):\n",
        "    \"\"\"Save batch results with error handling\"\"\"\n",
        "    try:\n",
        "        # Create DataFrame for this batch\n",
        "        words = list(batch_embeddings.keys())\n",
        "        embeddings_matrix = np.stack([batch_embeddings[word] for word in words])\n",
        "        similarity_matrix = np.dot(embeddings_matrix, embeddings_matrix.T)\n",
        "\n",
        "        results = {\n",
        "            'pointer': words,\n",
        "            'category': [batch_map[word] for word in words],\n",
        "            'embedding': [batch_embeddings[word] for word in words]\n",
        "        }\n",
        "        batch_df = pd.DataFrame(results)\n",
        "\n",
        "        # Save locally with timestamp\n",
        "        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
        "        local_path_df = f'/content/batch_{batch_id}_{category_name}_{timestamp}_embeddings.pkl'\n",
        "        local_path_matrix = f'/content/batch_{batch_id}_{category_name}_{timestamp}_similarity.npy'\n",
        "\n",
        "        batch_df.to_pickle(local_path_df)\n",
        "        np.save(local_path_matrix, similarity_matrix)\n",
        "\n",
        "        # Save to GCS\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(\"eliteness\")\n",
        "\n",
        "        for local_file, file_type in [\n",
        "            (local_path_df, 'embeddings'),\n",
        "            (local_path_matrix, 'similarity')\n",
        "        ]:\n",
        "            blob = bucket.blob(f'embeddings_analysis/batches/batch_{batch_id}_{category_name}_{timestamp}_{file_type}')\n",
        "            blob.upload_from_filename(local_file)\n",
        "\n",
        "        print(f\"Saved batch {batch_id} ({category_name}) with {len(words)} pointers\")\n",
        "\n",
        "        # Clear local files\n",
        "        os.remove(local_path_df)\n",
        "        os.remove(local_path_matrix)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving batch {batch_id}: {str(e)}\")\n",
        "        # Try to save locally only\n",
        "        try:\n",
        "            batch_df.to_pickle(f'/content/BACKUP_batch_{batch_id}_{category_name}.pkl')\n",
        "            print(f\"Saved backup to local storage\")\n",
        "        except:\n",
        "            print(\"Failed to save backup\")\n",
        "\n",
        "def check_processed_batches():\n",
        "    \"\"\"Check which batches and subcategories have already been processed\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(\"eliteness\")\n",
        "\n",
        "    # Track at subcategory level\n",
        "    processed_items = defaultdict(dict)\n",
        "    existing_batches = set()\n",
        "\n",
        "    print(\"\\nChecking existing files in bucket...\")\n",
        "    for blob in bucket.list_blobs(prefix='embeddings_analysis/batches/'):\n",
        "        filename = blob.name.split('/')[-1]\n",
        "        if not filename.startswith('batch_'):\n",
        "            continue\n",
        "\n",
        "        parts = filename.split('_')\n",
        "        if len(parts) >= 4:\n",
        "            batch_num = parts[1]\n",
        "            category_type = parts[2]  # e.g., 'entities', 'public'\n",
        "            subcategory = parts[3]    # e.g., 'organization'\n",
        "\n",
        "            # Store the complete batch identifier\n",
        "            batch_key = f\"{batch_num}_{category_type}_{subcategory}\"\n",
        "            existing_batches.add(batch_key)\n",
        "\n",
        "            processed_items[batch_key] = {\n",
        "                'filename': filename,\n",
        "                'timestamp': parts[-2] if len(parts) > 4 else None,\n",
        "                'processed': True\n",
        "            }\n",
        "\n",
        "    if existing_batches:\n",
        "        print(\"\\nFound existing batches:\")\n",
        "        for batch in sorted(existing_batches):\n",
        "            print(f\"- {batch}\")\n",
        "    else:\n",
        "        print(\"\\nNo existing batches found\")\n",
        "\n",
        "    return processed_items\n",
        "\n",
        "def process_subcategory_batch(category_name, subcat, items, batch_id, processed_items, sub_batch_size=1000):\n",
        "    \"\"\"Process a subcategory in smaller sub-batches with better tracking\"\"\"\n",
        "    sub_batch_embeddings = {}\n",
        "    sub_batch_map = {}\n",
        "\n",
        "    total_sub_batches = (len(items) + sub_batch_size - 1) // sub_batch_size\n",
        "\n",
        "    for i in range(0, len(items), sub_batch_size):\n",
        "        sub_batch = items[i:i + sub_batch_size]\n",
        "        sub_batch_id = i // sub_batch_size\n",
        "\n",
        "        # Check if this specific sub-batch was processed\n",
        "        key = f\"{batch_id}_{category_name}_{subcat}_{sub_batch_id}\"\n",
        "        if key in processed_items:\n",
        "            print(f\"Skipping already processed sub-batch: {key}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nProcessing {key} ({len(sub_batch)} items) - Sub-batch {sub_batch_id + 1}/{total_sub_batches}\")\n",
        "\n",
        "        current_batch_embeddings = {}\n",
        "        current_batch_map = {}\n",
        "\n",
        "        for item in tqdm(sub_batch):\n",
        "            try:\n",
        "                embedding = get_word_embedding(item)\n",
        "                current_batch_embeddings[item] = embedding\n",
        "                current_batch_map[item] = f\"{category_name}_{subcat}\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {item}: {str(e)}\")\n",
        "\n",
        "        # Save current sub-batch\n",
        "        if current_batch_embeddings:\n",
        "            save_batch_results(\n",
        "                current_batch_embeddings,\n",
        "                current_batch_map,\n",
        "                f\"{batch_id}_{subcat}_{sub_batch_id}\",\n",
        "                category_name\n",
        "            )\n",
        "\n",
        "            # Update main dictionaries\n",
        "            sub_batch_embeddings.update(current_batch_embeddings)\n",
        "            sub_batch_map.update(current_batch_map)\n",
        "\n",
        "            # Clear memory\n",
        "            del current_batch_embeddings\n",
        "            del current_batch_map\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "    return sub_batch_embeddings, sub_batch_map\n",
        "\n",
        "def process_all_categories(resume=True):\n",
        "    \"\"\"Process categories with improved tracking\"\"\"\n",
        "    categories = load_categories()\n",
        "    processed_items = check_processed_batches() if resume else {}\n",
        "\n",
        "    # Initialize collection dictionaries\n",
        "    all_embeddings = {}\n",
        "    all_category_maps = {}\n",
        "\n",
        "    # Check if we've completed all categories\n",
        "    total_categories = len(categories.keys())\n",
        "    max_batch_found = -1\n",
        "    for key in processed_items.keys():\n",
        "        try:\n",
        "            batch_num = int(key.split('_')[0])\n",
        "            max_batch_found = max(max_batch_found, batch_num)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if max_batch_found >= total_categories - 1:  # -1 because batches start at 0\n",
        "        print(f\"\\nAll categories have been processed (found {max_batch_found + 1} batches for {total_categories} categories)\")\n",
        "        print(\"Processing complete!\")\n",
        "        return all_embeddings, all_category_maps\n",
        "\n",
        "    print(f\"\\nContinuing from batch {max_batch_found + 1} of {total_categories} total categories\")\n",
        "\n",
        "    # Continue with processing...\n",
        "    for batch_id, (category_name, subcategories) in enumerate(categories.items(), start=max_batch_found + 1):\n",
        "        if batch_id >= total_categories:\n",
        "            break\n",
        "\n",
        "        print(f\"\\nProcessing category: {category_name} (Batch {batch_id})\")\n",
        "\n",
        "        for subcat, items in subcategories.items():\n",
        "            if isinstance(items, list) and items:\n",
        "                key_prefix = f\"{batch_id}_{category_name}_{subcat}\"\n",
        "\n",
        "                if key_prefix not in processed_items:\n",
        "                    print(f\"\\nProcessing subcategory: {subcat} ({len(items)} items)\")\n",
        "                    sub_embeddings, sub_map = process_subcategory_batch(\n",
        "                        category_name, subcat, items, batch_id, processed_items\n",
        "                    )\n",
        "                    if sub_embeddings and sub_map:\n",
        "                        all_embeddings.update(sub_embeddings)\n",
        "                        all_category_maps.update(sub_map)\n",
        "                else:\n",
        "                    print(f\"\\nSkipping already processed: {key_prefix}\")\n",
        "\n",
        "    print(\"\\nProcessing complete!\")\n",
        "    return all_embeddings, all_category_maps\n",
        "\n",
        "def load_batch_results(batch_id, category_name):\n",
        "    \"\"\"Load results for a specific batch\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(\"eliteness\")\n",
        "\n",
        "    # Find the latest version of this batch's files\n",
        "    batch_files = []\n",
        "    for blob in bucket.list_blobs(prefix=f'embeddings_analysis/batches/batch_{batch_id}_{category_name}'):\n",
        "        batch_files.append(blob.name)\n",
        "\n",
        "    if not batch_files:\n",
        "        raise Exception(f\"No files found for batch {batch_id}\")\n",
        "\n",
        "    # Get latest version based on timestamp\n",
        "    latest_embedding_file = max([f for f in batch_files if 'embeddings' in f])\n",
        "\n",
        "    # Download and load\n",
        "    blob = bucket.blob(latest_embedding_file)\n",
        "    blob.download_to_filename('/content/temp_batch.pkl')\n",
        "    batch_df = pd.read_pickle('/content/temp_batch.pkl')\n",
        "\n",
        "    # Convert back to dictionary format\n",
        "    embeddings = dict(zip(batch_df['pointer'], batch_df['embedding']))\n",
        "    category_map = dict(zip(batch_df['pointer'], batch_df['category']))\n",
        "\n",
        "    # Clean up\n",
        "    os.remove('/content/temp_batch.pkl')\n",
        "\n",
        "    print(f\"Loaded {len(embeddings)} pointers from existing batch {batch_id}\")\n",
        "    return embeddings, category_map\n",
        "\n",
        "def main(resume=True):\n",
        "    print(\"Starting batch processing of categories...\")\n",
        "    if resume:\n",
        "        print(\"Checking for existing progress...\")\n",
        "\n",
        "    all_embeddings, all_category_maps = process_all_categories(resume=resume)\n",
        "    return all_embeddings, all_category_maps\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    all_embeddings, all_category_maps = main(resume=True)\n"
      ],
      "metadata": {
        "id": "Cx7rA1tZSreM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings cleaning and reorganizing"
      ],
      "metadata": {
        "id": "__mIeYxYS9Ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def clean_and_reorganize_embeddings():\n",
        "    \"\"\"Clean embedding files and save to new organized structure\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(\"eliteness\")\n",
        "\n",
        "    # Create a directory for temporary files\n",
        "    temp_dir = '/content/temp_clean'\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Track unique pointers and their embeddings\n",
        "    unique_embeddings = {}\n",
        "    file_mappings = defaultdict(list)\n",
        "\n",
        "    print(\"Scanning for embedding files...\")\n",
        "    embedding_files = [\n",
        "        blob.name for blob in bucket.list_blobs(prefix='embeddings_analysis/batches/')\n",
        "        if '_embeddings' in blob.name and '_similarity' not in blob.name\n",
        "    ]\n",
        "\n",
        "    print(f\"Found {len(embedding_files)} files to process\")\n",
        "\n",
        "    # Process files and collect unique embeddings\n",
        "    for file_name in tqdm(embedding_files, desc=\"Processing files\"):\n",
        "        try:\n",
        "            # Download and load file\n",
        "            temp_file = os.path.join(temp_dir, 'temp_batch.pkl')\n",
        "            blob = bucket.blob(file_name)\n",
        "            blob.download_to_filename(temp_file)\n",
        "            batch_df = pd.read_pickle(temp_file)\n",
        "            os.remove(temp_file)  # Clean up immediately after use\n",
        "\n",
        "            # Extract categories from filename\n",
        "            parts = file_name.split('/')[-1].split('_')\n",
        "            category_start = 2\n",
        "            main_category = '_'.join([p for p in parts[category_start:]\n",
        "                                   if not p.isdigit() and p not in ['embeddings', 'similarity']])\n",
        "\n",
        "            sub_types = ['titles', 'orgs', 'keywords', 'entities', 'public', 'private']\n",
        "            sub_category = next((p for p in parts if p in sub_types), 'unknown')\n",
        "\n",
        "            # Process each row\n",
        "            for _, row in batch_df.iterrows():\n",
        "                pointer = row['pointer']\n",
        "                if pointer not in unique_embeddings:\n",
        "                    unique_embeddings[pointer] = {\n",
        "                        'embedding': row['embedding'],\n",
        "                        'category': row['category'],\n",
        "                        'main_category': main_category,\n",
        "                        'sub_category': sub_category\n",
        "                    }\n",
        "                    file_mappings[f\"{main_category}_{sub_category}\"].append(pointer)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nFound {len(unique_embeddings)} unique pointers\")\n",
        "\n",
        "    # Save cleaned embeddings by category\n",
        "    print(\"\\nSaving cleaned embeddings...\")\n",
        "\n",
        "    for category, pointers in file_mappings.items():\n",
        "        try:\n",
        "            # Create DataFrame for this category\n",
        "            category_data = {\n",
        "                'pointer': [],\n",
        "                'embedding': [],\n",
        "                'category': [],\n",
        "                'main_category': [],\n",
        "                'sub_category': []\n",
        "            }\n",
        "\n",
        "            for pointer in pointers:\n",
        "                data = unique_embeddings[pointer]\n",
        "                category_data['pointer'].append(pointer)\n",
        "                category_data['embedding'].append(data['embedding'])\n",
        "                category_data['category'].append(data['category'])\n",
        "                category_data['main_category'].append(data['main_category'])\n",
        "                category_data['sub_category'].append(data['sub_category'])\n",
        "\n",
        "            df = pd.DataFrame(category_data)\n",
        "\n",
        "            # Save locally first\n",
        "            temp_file = os.path.join(temp_dir, f'{category}_clean.pkl')\n",
        "            df.to_pickle(temp_file)\n",
        "\n",
        "            # Upload to new folder in GCS\n",
        "            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
        "            blob = bucket.blob(f'embeddings_clean/{category}_{timestamp}.pkl')\n",
        "            blob.upload_from_filename(temp_file)\n",
        "\n",
        "            print(f\"Saved {len(df)} embeddings for {category}\")\n",
        "\n",
        "            # Clean up local file\n",
        "            os.remove(temp_file)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving {category}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Clean up temp directory - now should be empty\n",
        "    import shutil\n",
        "    shutil.rmtree(temp_dir)\n",
        "\n",
        "    print(\"\\nCleaning complete!\")\n",
        "    return len(unique_embeddings)\n",
        "\n",
        "def verify_cleaned_embeddings():\n",
        "    \"\"\"Verify the cleaned embeddings\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(\"eliteness\")\n",
        "\n",
        "    print(\"\\nVerifying cleaned embeddings...\")\n",
        "    clean_files = [\n",
        "        blob.name for blob in bucket.list_blobs(prefix='embeddings_clean/')\n",
        "    ]\n",
        "\n",
        "    total_embeddings = 0\n",
        "    categories = set()\n",
        "\n",
        "    for file_name in clean_files:\n",
        "        try:\n",
        "            blob = bucket.blob(file_name)\n",
        "            blob.download_to_filename('/content/verify_temp.pkl')\n",
        "            df = pd.read_pickle('/content/verify_temp.pkl')\n",
        "\n",
        "            total_embeddings += len(df)\n",
        "            categories.add(file_name.split('/')[-1].split('_')[0])\n",
        "\n",
        "            os.remove('/content/verify_temp.pkl')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error verifying {file_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nVerification Results:\")\n",
        "    print(f\"Total clean files: {len(clean_files)}\")\n",
        "    print(f\"Total embeddings: {total_embeddings}\")\n",
        "    print(f\"Categories: {sorted(categories)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting embedding cleanup process...\")\n",
        "    unique_count = clean_and_reorganize_embeddings()\n",
        "    verify_cleaned_embeddings()"
      ],
      "metadata": {
        "id": "-FdN42JXS9W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifier training"
      ],
      "metadata": {
        "id": "U7WwZYoHTOGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifier trainer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "\n",
        "def combine_batch_embeddings():\n",
        "    \"\"\"Load and combine batch embeddings with memory management and duplicate prevention\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(\"eliteness\")\n",
        "\n",
        "    # Track processed files and unique embeddings\n",
        "    processed_files = set()\n",
        "    unique_pointers = set()\n",
        "    all_embeddings = []\n",
        "    total_count = 0\n",
        "\n",
        "    print(\"Scanning for embedding files...\")\n",
        "    embedding_files = [\n",
        "        blob.name for blob in bucket.list_blobs(prefix='embeddings_analysis/batches/')\n",
        "        if '_embeddings' in blob.name and '_similarity' not in blob.name\n",
        "    ]\n",
        "\n",
        "    print(f\"Found {len(embedding_files)} embedding files\")\n",
        "    batch_size = 10  # Process 10 files at a time\n",
        "\n",
        "    for i in range(0, len(embedding_files), batch_size):\n",
        "        batch_files = embedding_files[i:i + batch_size]\n",
        "        batch_embeddings = []\n",
        "\n",
        "        print(f\"\\nProcessing batch {i//batch_size + 1} of {(len(embedding_files) + batch_size - 1)//batch_size}\")\n",
        "\n",
        "        for file_name in batch_files:\n",
        "            try:\n",
        "                if file_name in processed_files:\n",
        "                    continue\n",
        "\n",
        "                blob = bucket.blob(file_name)\n",
        "                blob.download_to_filename('/content/temp_batch.pkl')\n",
        "                batch_df = pd.read_pickle('/content/temp_batch.pkl')\n",
        "\n",
        "                # Check for duplicate pointers\n",
        "                new_pointers = set(batch_df['pointer'])\n",
        "                duplicate_count = len(new_pointers & unique_pointers)\n",
        "                if duplicate_count > 0:\n",
        "                    print(f\"Found {duplicate_count} duplicates in {file_name}\")\n",
        "                    batch_df = batch_df[~batch_df['pointer'].isin(unique_pointers)]\n",
        "\n",
        "                if len(batch_df) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Update unique pointers\n",
        "                unique_pointers.update(new_pointers)\n",
        "\n",
        "                # Extract categories\n",
        "                parts = file_name.split('/')[-1].split('_')\n",
        "                category_start = 2\n",
        "                main_category = '_'.join([p for p in parts[category_start:]\n",
        "                                       if not p.isdigit() and p not in ['embeddings', 'similarity']])\n",
        "\n",
        "                sub_types = ['titles', 'orgs', 'keywords', 'entities', 'public', 'private']\n",
        "                sub_category = next((p for p in parts if p in sub_types), 'unknown')\n",
        "\n",
        "                # Add category information\n",
        "                batch_df['main_category'] = main_category\n",
        "                batch_df['sub_category'] = sub_category\n",
        "\n",
        "                batch_embeddings.append(batch_df)\n",
        "                processed_files.add(file_name)\n",
        "\n",
        "                print(f\"Added {len(batch_df)} unique embeddings from {sub_category} in {main_category}\")\n",
        "                total_count += len(batch_df)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_name}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Combine batch and clear memory\n",
        "        if batch_embeddings:\n",
        "            combined_batch = pd.concat(batch_embeddings, ignore_index=True)\n",
        "            all_embeddings.append(combined_batch)\n",
        "\n",
        "        # Clear memory\n",
        "        del batch_embeddings\n",
        "        import gc\n",
        "        gc.collect()\n",
        "\n",
        "    # Final combination\n",
        "    if not all_embeddings:\n",
        "        raise ValueError(\"No embedding files were successfully loaded!\")\n",
        "\n",
        "    final_df = pd.concat(all_embeddings, ignore_index=True)\n",
        "    print(f\"\\nFinal Statistics:\")\n",
        "    print(f\"Total unique embeddings: {len(final_df)}\")\n",
        "    print(f\"Main categories: {sorted(final_df['main_category'].unique())}\")\n",
        "    print(f\"Sub-categories: {sorted(final_df['sub_category'].unique())}\")\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def train_hierarchical_classifier(data_df, batch_size=10000):\n",
        "    \"\"\"Train classifier with memory-efficient batching\"\"\"\n",
        "    # Convert embeddings to numpy array in batches\n",
        "    print(\"Converting embeddings to numpy array...\")\n",
        "    X_batches = []\n",
        "    for i in range(0, len(data_df), batch_size):\n",
        "        batch = data_df.iloc[i:i + batch_size]\n",
        "        X_batch = np.stack(batch['embedding'].values)\n",
        "        X_batches.append(X_batch)\n",
        "\n",
        "    X = np.concatenate(X_batches)\n",
        "    y_main = data_df['main_category'].values\n",
        "\n",
        "    print(f\"Prepared {len(X)} samples for training\")\n",
        "\n",
        "    # Continue with classifier training as before...\n",
        "    # [rest of the training code remains the same]\n",
        "\n",
        "def train_hierarchical_classifier(data_df, batch_size=10000, min_samples=2):\n",
        "    \"\"\"Train classifier with memory-efficient batching and minimum sample filtering\"\"\"\n",
        "    print(\"\\nPreparing training data...\")\n",
        "\n",
        "    # Filter categories with too few samples\n",
        "    category_counts = data_df['main_category'].value_counts()\n",
        "    valid_categories = category_counts[category_counts >= min_samples].index\n",
        "    print(f\"\\nFound {len(valid_categories)} categories with {min_samples}+ samples\")\n",
        "    print(\"Removing categories:\", set(data_df['main_category'].unique()) - set(valid_categories))\n",
        "\n",
        "    filtered_df = data_df[data_df['main_category'].isin(valid_categories)].copy()\n",
        "\n",
        "    # Convert embeddings to numpy array in batches\n",
        "    print(\"\\nConverting embeddings to numpy array...\")\n",
        "    X_batches = []\n",
        "    for i in range(0, len(filtered_df), batch_size):\n",
        "        batch = filtered_df.iloc[i:i + batch_size]\n",
        "        X_batch = np.stack(batch['embedding'].values)\n",
        "        X_batches.append(X_batch)\n",
        "\n",
        "    X = np.concatenate(X_batches)\n",
        "    y_main = filtered_df['main_category'].values\n",
        "\n",
        "    print(f\"\\nPrepared {len(X)} samples for training\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_main, test_size=0.2, random_state=42, stratify=y_main\n",
        "    )\n",
        "\n",
        "    # Train main classifier\n",
        "    print(\"\\nTraining main category classifier...\")\n",
        "    main_classifier = LogisticRegression(\n",
        "        multi_class='multinomial',\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "    main_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate main classifier\n",
        "    print(\"\\nMain Category Classification:\")\n",
        "    print(\"Cross-validation scores:\")\n",
        "    cv_scores = cross_val_score(main_classifier, X, y_main, cv=5)\n",
        "    print(f\"Mean CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "    y_pred = main_classifier.predict(X_test)\n",
        "    print(\"\\nTest set performance:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Train subcategory classifiers\n",
        "    sub_classifiers = {}\n",
        "    for main_cat in valid_categories:\n",
        "        cat_mask = filtered_df['main_category'] == main_cat\n",
        "        cat_data = filtered_df[cat_mask]\n",
        "\n",
        "        # Check if subcategories have enough samples\n",
        "        subcat_counts = cat_data['sub_category'].value_counts()\n",
        "        valid_subcats = subcat_counts[subcat_counts >= min_samples].index\n",
        "\n",
        "        if len(valid_subcats) > 1:  # Need at least 2 valid subcategories\n",
        "            print(f\"\\nTraining {main_cat} subcategory classifier...\")\n",
        "\n",
        "            sub_X = np.stack(cat_data[cat_data['sub_category'].isin(valid_subcats)]['embedding'].values)\n",
        "            sub_y = cat_data[cat_data['sub_category'].isin(valid_subcats)]['sub_category']\n",
        "\n",
        "            try:\n",
        "                sub_clf = LogisticRegression(\n",
        "                    multi_class='multinomial',\n",
        "                    max_iter=1000,\n",
        "                    class_weight='balanced'\n",
        "                )\n",
        "                sub_clf.fit(sub_X, sub_y)\n",
        "                sub_classifiers[main_cat] = sub_clf\n",
        "\n",
        "                # Evaluate if enough samples\n",
        "                if len(sub_y) >= 10:  # Minimum for 5-fold CV\n",
        "                    cv_scores = cross_val_score(sub_clf, sub_X, sub_y, cv=min(5, len(sub_y)))\n",
        "                    print(f\"Mean CV score: {cv_scores.mean():.3f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error training subcategory classifier for {main_cat}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    return {\n",
        "        'main_classifier': main_classifier,\n",
        "        'sub_classifiers': sub_classifiers,\n",
        "        'valid_categories': valid_categories\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # Load embeddings\n",
        "    print(\"Loading and combining embeddings...\")\n",
        "    data_df = combine_batch_embeddings()\n",
        "\n",
        "    # Train classifiers\n",
        "    classifiers = train_hierarchical_classifier(data_df)\n",
        "\n",
        "    # Save classifiers\n",
        "    save_classifiers(classifiers)\n",
        "\n",
        "    return classifiers, data_df\n",
        "\n",
        "def save_classifiers(classifiers, prefix='eliteness'):\n",
        "    \"\"\"Save trained classifiers to GCS\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(\"eliteness\")\n",
        "\n",
        "    # Save locally first\n",
        "    with open(f'/content/{prefix}_classifiers.pkl', 'wb') as f:\n",
        "        pickle.dump(classifiers, f)\n",
        "\n",
        "    # Upload to GCS\n",
        "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
        "    blob = bucket.blob(f'classifiers/{prefix}_classifier_{timestamp}.pkl')\n",
        "    blob.upload_from_filename(f'/content/{prefix}_classifiers.pkl')\n",
        "    print(f\"Saved classifiers to GCS\")\n",
        "\n",
        "def predict_eliteness(text, classifiers, tokenizer, model):\n",
        "    \"\"\"Predict eliteness categories for new text\"\"\"\n",
        "    # Get embedding\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state[0][1].numpy()\n",
        "\n",
        "    # Predict main category\n",
        "    main_category = classifiers['main_classifier'].predict([embedding])[0]\n",
        "\n",
        "    # Predict subcategory if available\n",
        "    sub_category = None\n",
        "    if main_category in classifiers['sub_classifiers']:\n",
        "        sub_category = classifiers['sub_classifiers'][main_category].predict([embedding])[0]\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'main_category': main_category,\n",
        "        'sub_category': sub_category\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # Load and combine embeddings\n",
        "    data_df = combine_batch_embeddings()\n",
        "\n",
        "    # Train classifiers\n",
        "    classifiers = train_hierarchical_classifier(data_df)\n",
        "\n",
        "    # Save classifiers\n",
        "    save_classifiers(classifiers)\n",
        "\n",
        "    return classifiers, data_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    classifiers, data_df = main()"
      ],
      "metadata": {
        "id": "FXrOP5uaTNP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliteness classification and sentiment score estimate on data collected with Google News API.\n",
        "Change model version for comparison"
      ],
      "metadata": {
        "id": "sD-91ArRT3dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nyt data fra google api\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "from collections import defaultdict\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import torch\n",
        "import logging\n",
        "from google.cloud import storage\n",
        "import pickle\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "def load_classifier():\n",
        "    \"\"\"Load trained classifier from GCS\"\"\"\n",
        "    try:\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(\"eliteness\")\n",
        "\n",
        "        blobs = list(bucket.list_blobs(prefix='classifiers/'))\n",
        "        latest_classifier = max(blobs, key=lambda x: x.name)\n",
        "\n",
        "        latest_classifier.download_to_filename('/content/classifier.pkl')\n",
        "        with open('/content/classifier.pkl', 'rb') as f:\n",
        "            classifiers = pickle.load(f)\n",
        "\n",
        "        print(f\"Loaded classifier from {latest_classifier.name}\")\n",
        "        return classifiers\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading classifier: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "class NewsAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.classifiers = load_classifier()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"Maltehb/danish-bert-botxo\")\n",
        "        self.model = AutoModel.from_pretrained(\"Maltehb/danish-bert-botxo\")\n",
        "        self.nlp = spacy.load('da_core_news_lg')\n",
        "        self.sentiment_pipeline = pipeline(\"sentiment-analysis\",\n",
        "                                        model=\"DGurgurov/xlm-r_danish_sentiment\")\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean text by fixing encoding issues and removing truncation markers\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Fix common encoding issues\n",
        "        replacements = {\n",
        "            'Ã¸': 'ø',\n",
        "            'Ã¦': 'æ',\n",
        "            'Ã¥': 'å',\n",
        "            'Ã˜': 'Ø',\n",
        "            'Ã†': 'Æ',\n",
        "            'Ã…': 'Å'\n",
        "        }\n",
        "\n",
        "        for old, new in replacements.items():\n",
        "            text = text.replace(old, new)\n",
        "\n",
        "        # Remove truncation marker and everything after it\n",
        "        if '[+' in text and 'chars]' in text:\n",
        "            text = text.split('[+')[0].strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embedding = outputs.last_hidden_state[0][1].numpy()\n",
        "        return embedding\n",
        "\n",
        "    def classify_text(self, text: str) -> Dict:\n",
        "        \"\"\"Classify a piece of text using the trained classifier\"\"\"\n",
        "        try:\n",
        "            if not text:\n",
        "                return {'category': 'unknown', 'confidence': 0.0}\n",
        "\n",
        "            embedding = self.get_embedding(text)\n",
        "            main_classifier = self.classifiers['main_classifier']\n",
        "            pred_class = main_classifier.predict([embedding])[0]\n",
        "            confidence = np.max(main_classifier.predict_proba([embedding])[0])\n",
        "\n",
        "            return {'category': pred_class, 'confidence': confidence}\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in text classification: {str(e)}\")\n",
        "            return {'category': 'unknown', 'confidence': 0.0}\n",
        "\n",
        "    def analyze_sentiment(self, text: str) -> Dict:\n",
        "        \"\"\"Analyze sentiment of cleaned text\"\"\"\n",
        "        try:\n",
        "            if not text:\n",
        "                return {'label': 'L', 'confidence': 0.0}\n",
        "\n",
        "            cleaned_text = self.clean_text(text)\n",
        "            result = self.sentiment_pipeline(cleaned_text[:512])[0]\n",
        "\n",
        "            label_map = {\n",
        "                'LABEL_1': 'P',\n",
        "                'LABEL_0': 'N'\n",
        "            }\n",
        "            return {\n",
        "                'label': label_map.get(result['label'], 'L'),\n",
        "                'confidence': result['score']\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in sentiment analysis: {str(e)}\")\n",
        "            return {'label': 'L', 'confidence': 0.0}\n",
        "\n",
        "    def get_combined_sentiment(self, sentiments: List[Dict]) -> Dict:\n",
        "        \"\"\"Calculate combined sentiment from multiple analyses\"\"\"\n",
        "        valid_sentiments = [s for s in sentiments if s['label'] != 'L']\n",
        "        if not valid_sentiments:\n",
        "            return {'label': 'L', 'confidence': 0.0}\n",
        "\n",
        "        # Weight sentiments by confidence\n",
        "        weighted_sum = sum(1 if s['label'] == 'P' else -1 * s['confidence']\n",
        "                         for s in valid_sentiments)\n",
        "        avg_confidence = sum(s['confidence'] for s in valid_sentiments) / len(valid_sentiments)\n",
        "\n",
        "        return {\n",
        "            'label': 'P' if weighted_sum > 0 else 'N',\n",
        "            'confidence': avg_confidence\n",
        "        }\n",
        "\n",
        "    def analyze_article(self, title: str, description: str, content: str) -> Dict:\n",
        "        \"\"\"Analyze full article including title, description, and content\"\"\"\n",
        "        # Clean texts\n",
        "        clean_title = self.clean_text(title)\n",
        "        clean_desc = self.clean_text(description)\n",
        "        clean_content = self.clean_text(content)\n",
        "\n",
        "        # Split content into sentences\n",
        "        doc = self.nlp(clean_content)\n",
        "        sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "        # Classify each sentence\n",
        "        category_counts = defaultdict(int)\n",
        "\n",
        "        # Classify title\n",
        "        title_class = self.classify_text(clean_title)\n",
        "        category_counts[title_class['category']] += 1\n",
        "\n",
        "        # Classify each sentence\n",
        "        for sentence in sentences:\n",
        "            classification = self.classify_text(sentence)\n",
        "            category_counts[classification['category']] += 1\n",
        "\n",
        "        # Get primary categories based on counts\n",
        "        primary_category = max(category_counts.items(), key=lambda x: x[1])\n",
        "\n",
        "        # Get sentiment for each part\n",
        "        sentiments = {\n",
        "            'title': self.analyze_sentiment(clean_title),\n",
        "            'description': self.analyze_sentiment(clean_desc),\n",
        "            'content': self.analyze_sentiment(clean_content)\n",
        "        }\n",
        "\n",
        "        # Calculate combined sentiment\n",
        "        combined_sentiment = self.get_combined_sentiment(list(sentiments.values()))\n",
        "        sentiments['combined'] = combined_sentiment\n",
        "\n",
        "        return {\n",
        "            'category_counts': dict(category_counts),\n",
        "            'primary_category': primary_category[0],\n",
        "            'category_frequency': primary_category[1],\n",
        "            'sentiment': sentiments\n",
        "        }\n",
        "\n",
        "def analyze_news_dataset(file_path: str) -> Dict:\n",
        "    analyzer = NewsAnalyzer()\n",
        "    results = {}\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Analyzing {len(df)} articles...\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        print(f\"Processing article {idx + 1}/{len(df)}\")\n",
        "        results[idx] = {\n",
        "            'source': row['source'],\n",
        "            'title': row['title'],\n",
        "            'publishedAt': row['publishedAt'],\n",
        "            'analysis': analyzer.analyze_article(\n",
        "                row['title'],\n",
        "                row['description'],\n",
        "                row['content']\n",
        "            )\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "def display_results(results: Dict):\n",
        "    for article_id, data in results.items():\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Source: {data['source']}\")\n",
        "        print(f\"Title: {data['title']}\")\n",
        "        print(f\"Date: {data['publishedAt']}\")\n",
        "\n",
        "        analysis = data['analysis']\n",
        "        print(f\"\\nPrimary Category: {analysis['primary_category']} \"\n",
        "              f\"(frequency: {analysis['category_frequency']})\")\n",
        "\n",
        "        print(\"\\nCategory Distribution:\")\n",
        "        for category, count in sorted(analysis['category_counts'].items(),\n",
        "                                    key=lambda x: x[1], reverse=True):\n",
        "            print(f\"{category}: {count}\")\n",
        "\n",
        "        print(\"\\nSentiment Analysis:\")\n",
        "        sentiments = analysis['sentiment']\n",
        "        for part, sent in sentiments.items():\n",
        "            print(f\"{part.capitalize()}: {sent['label']} \"\n",
        "                  f\"(confidence: {sent['confidence']:.2f})\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    results = analyze_news_dataset('/content/drive/MyDrive/NewsMarketAnalysis/danish_news_multiple_topics.csv')\n",
        "    display_results(results)"
      ],
      "metadata": {
        "id": "jdqlyFgfT3q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inspection of document categories\n",
        "import json\n",
        "\n",
        "def inspect_categories(json_file_path):\n",
        "    \"\"\"\n",
        "    Inspect the categories and their associated keys in the given JSON document.\n",
        "    :param json_file_path: Path to the JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the JSON file\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        print(\"Categories and their associated keys:\\n\")\n",
        "\n",
        "        # Iterate through the categories\n",
        "        for category, content in data.items():\n",
        "            print(f\"Category: {category}\")\n",
        "            if isinstance(content, dict):\n",
        "                print(f\"  Keys: {list(content.keys())}\")\n",
        "            else:\n",
        "                print(f\"  Content is not a dictionary (type: {type(content)}).\")\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while processing the file: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Path to the input JSON file\n",
        "    input_file = \"/content/drive/MyDrive/NewsData/processed/ElitenessBatchesValidation/merged_results_20241216_181011.json\"\n",
        "\n",
        "    #validation_results_path = \"/content/drive/MyDrive/NewsData/processed/ElitenessBatchesValidation/merged_results_20241216_181011.json\"\n",
        "    #training_results_path = \"/content/drive/MyDrive/NewsData/processed/ElitenessBatchesTrain/merged_results_20241215_203840.json\"\n",
        "\n",
        "    # Inspect categories\n",
        "    inspect_categories(input_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvU_5L_uiFWj",
        "outputId": "deb0a543-d2fb-4c4d-e53c-5c9594d0ebbc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories and their associated keys:\n",
            "\n",
            "Category: market_categories\n",
            "  Keys: ['energy_and_green_transition', 'welfare', 'maritime_and_shipping', 'agriculture_food', 'union_labour', 'real_estate', 'finance', 'industry', 'tech', 'regulatory', 'education', 'healthcare', 'politics', 'aviation', 'design', 'architecture', 'hospitality', 'tourism', 'appliances']\n",
            "\n",
            "Category: elite_hierarchy\n",
            "  Keys: []\n",
            "\n",
            "Category: org_forms\n",
            "  Keys: []\n",
            "\n",
            "Category: international_markers\n",
            "  Keys: []\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def extract_market_categories_details(json_file_path):\n",
        "    \"\"\"\n",
        "    Extract details for each subcategory under 'market_categories' and display their keys and values.\n",
        "    :param json_file_path: Path to the JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the JSON file\n",
        "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Check if 'market_categories' exists\n",
        "        if 'market_categories' not in data:\n",
        "            print(\"'market_categories' not found in the JSON file.\")\n",
        "            return\n",
        "\n",
        "        # Extract subcategories and their details\n",
        "        market_categories = data['market_categories']\n",
        "\n",
        "        print(\"Details for each category in 'market_categories':\\n\")\n",
        "        for subcategory, content in market_categories.items():\n",
        "            print(f\"- {subcategory}:\")\n",
        "            if isinstance(content, dict):\n",
        "                for key, values in content.items():\n",
        "                    print(f\"  - {key}: {values}\")\n",
        "            else:\n",
        "                print(f\"  - Content is not a dictionary (type: {type(content)}).\")\n",
        "            print()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while processing the file: {e}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Path to the input JSON file\n",
        "    input_file = \"/content/drive/MyDrive/NewsData/processed/ElitenessBatchesTrain/merged_results_20241215_203840.json\"\n",
        "\n",
        "    # Extract details for 'market_categories'\n",
        "    extract_market_categories_details(input_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeUjUA_WjnSt",
        "outputId": "ecff0e7b-d283-42b5-82b8-adf5e7350bf5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Details for each category in 'market_categories':\n",
            "\n",
            "- energy_and_green_transition:\n",
            "  - titles: ['Bæredygtighedschef', 'Klimachef', 'Klimadirektør', 'Miljøchef', 'VINDMØLLER', 'Vindmølle', 'Vindmøller', 'bæredygtighedschef', 'energidirektør', 'klimachef', 'klimadirektør', 'koncernmiljøchef', 'kraft værk', 'miljøchef', 'over vindmølle', 'vindmølle', 'vindmølleanalytiker', 'vindmøller', 'vindmølles']\n",
            "  - orgs: ['Energiselskab', 'Energiselskabs', 'Energistyrelse', 'Forsyning', 'Forsynings', 'KLIMA', 'KLIMAs', 'Klima', 'Klimachef', 'Klimadirektør', 'Klimarådgiver', 'MILJØ', 'MIljø', 'Miljø', 'Miljøchef', 'Miljødirektør', 'Miljøformand', 'Miljøkonsulent', 'Miljøs', 'Miljøstyrelse', 'Miljøstyrelses', 'VINDMØLLER', 'Vindmølle', 'Vindmøller', 'energiselskab', 'energiselskabs', 'energistyrelse', 'energistyrelses', 'forsyning', 'forsynings', 'først Miljø', 'gruppe miljø', 'klima', 'klimaanalytiker', 'klimaansvarlig', 'klimachef', 'klimadirektør', 'klimaformand', 'klimakonsulent', 'klimakoordinator', 'klimaleder', 'klimarådgiver', 'klimas', 'mellem Forsyning', 'mellem Klima', 'mellem Miljø', 'mellem klima', 'mellem miljø', 'miljø', 'miljøansvarlig', 'miljøchef', 'miljødirektør', 'miljøe', 'miljøformand', 'miljøkonsulent', 'miljøkoordinator', 'miljøleder', 'miljørådgiver', 'miljøs', 'miljøstyrelse', 'miljøstyrelses', 'over Miljø', 'over klima', 'over miljø', 'over vindmølle', 'overforsyning', 'specialmiljø', 'topmiljø', 'under Klima', 'under Miljø', 'under klima', 'under miljø', 'underforsyning', 'vindmølle', 'vindmølleanalytiker', 'vindmøller', 'vindmølles']\n",
            "  - keywords: ['BÆREDYGTIG', 'Bæredygtig', 'Bæredygtige', 'ENERGI', 'Energi', 'Energichef', 'Energie', 'Energiformand', 'Energirådgiver', 'Energis', 'Energispecialist', 'GRØN', 'GRøn', 'Grøn', 'Grøne', 'Grøns', 'KLIMA', 'KLIMAs', 'Klima', 'Klimachef', 'Klimadirektør', 'Klimarådgiver', 'MILJØ', 'MIljø', 'Miljø', 'Miljøchef', 'Miljødirektør', 'Miljøformand', 'Miljøkonsulent', 'Miljøs', 'OMSTILLING', 'Omstilling', 'Omstillings', 'Overenergi', 'POWER', 'Power', 'Powers', 'VINDKRAFT', 'Vedvarende', 'Vindkraft', 'bæredygtig', 'bæredygtige', 'bæredygtigs', 'energi', 'energianalytiker', 'energiansvarlig', 'energichef', 'energidirektør', 'energie', 'energikonsulent', 'energikoordinator', 'energileder', 'energirådgiver', 'energis', 'energispecialist', 'først Miljø', 'først grøn', 'gruppe miljø', 'grøn', 'grøne', 'klima', 'klimaanalytiker', 'klimaansvarlig', 'klimachef', 'klimadirektør', 'klimaformand', 'klimakonsulent', 'klimakoordinator', 'klimaleder', 'klimarådgiver', 'klimas', 'mellem Klima', 'mellem Miljø', 'mellem bæredygtig', 'mellem energi', 'mellem grøn', 'mellem klima', 'mellem miljø', 'mellem vindkraft', 'mellemgrøn', 'miljø', 'miljøansvarlig', 'miljøchef', 'miljødirektør', 'miljøe', 'miljøformand', 'miljøkonsulent', 'miljøkoordinator', 'miljøleder', 'miljørådgiver', 'miljøs', 'omstilling', 'omstillings', 'over Miljø', 'over energi', 'over grøn', 'over klima', 'over miljø', 'over vedvarende', 'power', 'powers', 'specialmiljø', 'topmiljø', 'under Energi', 'under Grøn', 'under Klima', 'under Miljø', 'under Omstilling', 'under bæredygtig', 'under energi', 'under klima', 'under miljø', 'under omstilling', 'under vedvarende', 'underenergi', 'vedvarende', 'vindkraft']\n",
            "\n",
            "- welfare:\n",
            "  - keywords: ['Beskæftigelse', 'Beskæftigelses', 'BØRN', 'BØRNE', 'Børn', 'Børne', 'Børns', 'INTEGRATION', 'Integration', 'Integrations', 'KOMMUNE', 'KOMMUNER', 'KOmmune', 'Kommune', 'Kommunekoordinator', 'Kommuner', 'Kommunes', 'OMSORG', 'Omsorg', 'Omsorgs', 'Over kommune', 'Regionkommune', 'SOCIAL', 'SOCIALE', 'Social', 'Socialchef', 'Socialdirektør', 'Sociale', 'Socialformand', 'Socialkonsulent', 'Socialrådgiver', 'Socials', 'Specialbørn', 'UNGE', 'UNGER', 'UNge', 'Unge', 'Ungechef', 'Ungedirektør', 'Ungeformand', 'Ungekoordinator', 'Unger', 'Ungerådgiver', 'Unges', 'VELFÆRD', 'Velfærd', 'Velfærds', 'beskæftigelse', 'beskæftigelser', 'beskæftigelses', 'børn', 'børne', 'børns', 'først børn', 'først velfærd', 'gruppe børn', 'gruppe social', 'gruppe unge', 'gruppe ældre', 'hovedbeskæftigelse', 'integration', 'integratione', 'integrations', 'kOMMUNE', 'kommune', 'kommunechef', 'kommunedirektør', 'kommunekoordinator', 'kommuner', 'kommunes', 'mellem Børn', 'mellem Kommune', 'mellem beskæftigelse', 'mellem børn', 'mellem integration', 'mellem kommune', 'mellem omsorg', 'mellem social', 'mellem unge', 'mellem velfærd', 'mellem Ældre', 'mellem ældre', 'mellembørn', 'omsorg', 'omsorgs', 'over Børn', 'over Social', 'over beskæftigelse', 'over børn', 'over integration', 'over social', 'over unge', 'over velfærd', 'over ældre', 'seniorkommune', 'social', 'socialansvarlig', 'socialchef', 'socialdirektør', 'sociale', 'socialkonsulent', 'socialkoordinator', 'socialleder', 'socialrådgiver', 'specialbørn', 'under Børn', 'under Social', 'under beskæftigelse', 'under børn', 'under integration', 'under social', 'under unge', 'under Ældre', 'under ældre', 'underbeskæftigelse', 'unge', 'ungechef', 'ungedirektør', 'ungeformand', 'ungekonsulent', 'ungekoordinator', 'ungeleder', 'unger', 'ungerådgiver', 'unges', 'velfærd', 'velfærds', 'ÆLDRE', 'ÆLdre', 'Ældre', 'Ældrechef', 'Ældredirektør', 'Ældreformand', 'Ældrekonsulent', 'Ældres', 'ældre', 'ældrechef', 'ældredirektør', 'ældrekonsulent', 'ældrer', 'ældrerådgiver', 'ældres']\n",
            "  - orgs: ['Beskæftigelse', 'Beskæftigelses', 'Børnehus', 'Børnehuse', 'KOMMUNE', 'KOMMUNER', 'KOmmune', 'Kommune', 'Kommunekoordinator', 'Kommuner', 'Kommunes', 'Over kommune', 'REGION', 'Region', 'Regione', 'Regionkommune', 'Regions', 'Socialministerium', 'Socialstyrelse', 'VELFÆRD', 'Velfærd', 'Velfærds', 'beskæftigelse', 'beskæftigelser', 'beskæftigelses', 'børnehus', 'børnehuse', 'børnehusleder', 'først Region', 'først region', 'først velfærd', 'hovedbeskæftigelse', 'kOMMUNE', 'kommune', 'kommunechef', 'kommunedirektør', 'kommunekoordinator', 'kommuner', 'kommunes', 'mellem Kommune', 'mellem Region', 'mellem beskæftigelse', 'mellem kommune', 'mellem region', 'mellem velfærd', 'over Region', 'over beskæftigelse', 'over region', 'over velfærd', 'region', 'regione', 'regions', 'seniorkommune', 'socialministerium', 'socialstyrelse', 'under Region', 'under beskæftigelse', 'under region', 'underbeskæftigelse', 'velfærd', 'velfærds', 'Ældrecenter', 'ældrecenter']\n",
            "  - titles: ['Beskæftigelseschef', 'Socialchef', 'Ungechef', 'Velfærdsdirektør', 'beskæftigelseschef', 'børnechef', 'integrationschef', 'socialchef', 'socialchefs', 'ungechef', 'velfærdsdirektør', 'Ældrechef', 'ældrechef']\n",
            "\n",
            "- maritime_and_shipping:\n",
            "  - orgs: ['Container', 'Containere', 'Containers', 'FRAGT', 'Fragt', 'HAVN', 'Havn', 'Havne', 'Havns', 'Maritime', 'Maritimes', 'OFFSHORE', 'Offshore', 'Offshores', 'Rederi', 'Rederidirektør', 'Rederiformand', 'Rederis', 'Shipping', 'Shippinganalytiker', 'Shippings', 'Specialrederi', 'Søfart', 'Søfarts', 'Værft', 'Værfts', 'container', 'containere', 'containers', 'fragt', 'fragte', 'havn', 'havne', 'havns', 'hovedhavn', 'maritime', 'mellem havn', 'mellem rederi', 'offshore', 'over havn', 'over offshore', 'rederi', 'rederichef', 'rederidirektør', 'rederis', 'shipping', 'shippinganalytiker', 'specialcontainer', 'søfart', 'søfarts', 'under fragt', 'under maritime', 'værft', 'værftleder', 'værfts']\n",
            "  - keywords: ['Container', 'Containere', 'Containers', 'FRAGT', 'Fragt', 'HAVN', 'Havn', 'Havne', 'Havns', 'Logistik', 'Logistikchef', 'Logistikdirektør', 'Logistiks', 'MARITIM', 'Maritim', 'Maritime', 'OFFSHORE', 'Offshore', 'Offshores', 'SKIB', 'SKIBE', 'Shipping', 'Shippinganalytiker', 'Shippings', 'Skib', 'Skibe', 'Skibs', 'Søfart', 'Søfarts', 'container', 'containere', 'containers', 'fragt', 'fragte', 'havn', 'havne', 'havns', 'hovedhavn', 'hovedskib', 'logistik', 'logistikchef', 'logistikdirektør', 'logistikkoordinator', 'logistikleder', 'maritim', 'maritimchef', 'maritime', 'mellem havn', 'mellem skib', 'offshore', 'over havn', 'over offshore', 'shipping', 'shippinganalytiker', 'skib', 'skibe', 'skibs', 'specialcontainer', 'specialskib', 'søfart', 'søfarts', 'under fragt']\n",
            "  - titles: ['Havnedirektør', 'Logistikchef', 'Reder', 'Reders', 'Skibsreder', 'Skibsredere', 'Skibsreders', 'havnedirektør', 'koncernlogistikchef', 'logistikchef', 'marinechef', 'mellem skibsreder', 'reder', 'redere', 'seniorreder', 'skibsreder', 'skibsredere', 'skibsreders', 'under reder']\n",
            "\n",
            "- agriculture_food:\n",
            "  - orgs: ['FØDEVARER', 'Fødevare', 'Fødevarechef', 'Fødevaredirektør', 'Fødevarer', 'Fødevares', 'Fødevarestyrelse', 'Gård', 'Gårde', 'Gårds', 'Hovedgård', 'LANDBRUG', 'Landbrug', 'Landbrugs', 'Landbrugsorganisation', 'Mejeri', 'Mejerichef', 'Mejerie', 'Mejeris', 'Mellemgård', 'Overgård', 'Slagteri', 'Slagteridirektør', 'fødevare', 'fødevarechef', 'fødevarekonsulent', 'fødevarer', 'fødevares', 'fødevarestyrelse', 'fødevarestyrelser', 'gård', 'gårde', 'gårds', 'hovedgård', 'landbrug', 'landbruge', 'landbrugs', 'landbrugsorganisation', 'landbrugsorganisations', 'mejeri', 'mejerichef', 'mejeridirektør', 'mejeris', 'mellem Landbrug', 'mellem fødevare', 'mellem landbrug', 'mellem slagteri', 'over fødevare', 'over landbrug', 'slagteri', 'speciallandbrug', 'under Landbrug']\n",
            "  - keywords: ['EKSPORT', 'Eksport', 'FØDEVARER', 'Fødevare', 'Fødevarechef', 'Fødevaredirektør', 'Fødevarer', 'Fødevares', 'Fødevaresikkerhed', 'Gård', 'Gårde', 'Gårds', 'Hovedgård', 'LANDBRUG', 'Landbrug', 'Landbrugs', 'MARK', 'Mark', 'Marke', 'Markformand', 'Marks', 'Mejeri', 'Mejerichef', 'Mejerie', 'Mejeris', 'Mellemgård', 'Overgård', 'Overmark', 'Topmark', 'chef Mark', 'eksport', 'eksportchef', 'eksportdirektør', 'eksportrådgiver', 'eksports', 'fødevare', 'fødevarechef', 'fødevarekonsulent', 'fødevarer', 'fødevares', 'fødevaresikkerhed', 'først Mark', 'general Mark', 'gård', 'gårde', 'gårds', 'hovedgård', 'landbrug', 'landbruge', 'landbrugs', 'mark', 'marke', 'markformand', 'marks', 'mejeri', 'mejerichef', 'mejeridirektør', 'mejeris', 'mellem Landbrug', 'mellem Mark', 'mellem eksport', 'mellem fødevare', 'mellem landbrug', 'mellem mark', 'mellem økologi', 'over Mark', 'over eksport', 'over fødevare', 'over fødevaresikkerhed', 'over landbrug', 'over mark', 'over økologi', 'speciallandbrug', 'under Landbrug', 'ØKOLOGI', 'Økologi', 'Økologichef', 'økologi', 'økologichef', 'økologikonsulent']\n",
            "  - titles: ['Fødevarechef', 'Fødevaredirektør', 'Gårdejer', 'Landmand', 'Landmands', 'Mejerichef', 'fødevarechef', 'først landmand', 'gårdejer', 'gårdejere', 'landmand', 'landmands', 'mejerichef', 'over gårdejer']\n",
            "\n",
            "- union_labour:\n",
            "  - keywords: ['ARBEJDSMARKED', 'Arbejdsmarked', 'Arbejdsmarkeds', 'Arbejdsret', 'Fagbevægelse', 'Fagbevægelser', 'Fagbevægelses', 'Faglig', 'Faglige', 'Forhandling', 'Først medlem', 'Medlem', 'Medlems', 'Overenskomst', 'arbejdsmarked', 'arbejdsmarkede', 'arbejdsmarkeds', 'arbejdsret', 'arbejdsrets', 'central overenskomst', 'fagbevægelse', 'fagbevægelser', 'fagbevægelses', 'faglig', 'faglige', 'fagligkonsulent', 'forhandling', 'forhandlings', 'først faglig', 'først medlem', 'gruppe medlem', 'gruppemedlem', 'hovedforhandling', 'hovedoverenskomst', 'juniormedlem', 'medlem', 'medlems', 'mellem fagbevægelse', 'mellem faglig', 'mellem overenskomst', 'overenskomst', 'overenskomstchef', 'seniorarbejdsmarked', 'seniormedlem', 'specialfaglig', 'topfaglig', 'topmedlem', 'under faglig', 'under forhandling', 'under overenskomst']\n",
            "  - orgs: ['Akasse', 'Arbejdsgiver', 'Arbejdsgivere', 'Centralforbund', 'FORBUND', 'Fagbevægelse', 'Fagbevægelser', 'Fagbevægelses', 'Fagforening', 'Fagforenings', 'Forbund', 'Forbunds', 'Hovedorganisation', 'Hovedorganisations', 'Overenskomst', 'Special forbund', 'akasse', 'akasseleder', 'akasser', 'arbejdsgiver', 'arbejdsgivere', 'arbejdsgivers', 'central overenskomst', 'fagbevægelse', 'fagbevægelser', 'fagbevægelses', 'fagforening', 'fagforeninge', 'fagforenings', 'forbund', 'forbunds', 'hovedforbund', 'hovedorganisation', 'hovedoverenskomst', 'mellem arbejdsgiver', 'mellem fagbevægelse', 'mellem fagforening', 'mellem forbund', 'mellem overenskomst', 'overenskomst', 'overenskomstchef', 'specialforbund', 'under overenskomst']\n",
            "  - titles: ['Arbejdsmarkedschef', 'Fagforeningsformand', 'Forbundsformand', 'arbejdsmarkedschef', 'fagforeningsformand', 'forbundsformand', 'forhandlingsleder', 'forhandlingsledere', 'hovedkasserer']\n",
            "\n",
            "- real_estate:\n",
            "  - orgs: ['Administration', 'Administrations', 'Boligforening', 'Boligforenings', 'Boligselskab', 'Boligselskabs', 'Centraladministration', 'Developer', 'Developers', 'Ejendomsadministration', 'Ejendomsselskab', 'Ejendomsselskabs', 'General Administration', 'Gruppeadministration', 'Under administration', 'administration', 'administrations', 'boligforening', 'boligforenings', 'boligselskab', 'boligselskabs', 'central administration', 'centraladministration', 'developer', 'developere', 'developers', 'ejendomsadministration', 'ejendomsadministrations', 'ejendomsselskab', 'gruppeadministration', 'hovedadministration', 'mellem administration', 'mellem boligforening', 'over administration', 'overadministration', 'under Administration', 'under administration']\n",
            "  - keywords: ['Administration', 'Administrations', 'BOLIG', 'BYGGERI', 'Bolig', 'Boligchef', 'Boligdirektør', 'Boligs', 'Byggeri', 'Byggeris', 'Bygherre', 'Bygherrer', 'Bygherrerådgiver', 'Centraladministration', 'Developer', 'Developers', 'Ejendom', 'Ejendoms', 'General Administration', 'Gruppeadministration', 'Investering', 'Investerings', 'UDLEJNING', 'Udlejning', 'Udlejnings', 'Under administration', 'administration', 'administrations', 'bolig', 'boligchef', 'bolige', 'boligformand', 'boligkonsulent', 'boligkoordinator', 'boligrådgiver', 'boligs', 'byggeri', 'byggerie', 'byggeris', 'bygherre', 'bygherreansvarlig', 'bygherrer', 'bygherrerådgiver', 'bygherres', 'central administration', 'centraladministration', 'developer', 'developere', 'developers', 'ejendom', 'ejendoms', 'gruppeadministration', 'hovedadministration', 'investering', 'investerings', 'mellem administration', 'mellem bolig', 'mellem bygherre', 'mellem investering', 'mellem udlejning', 'over administration', 'over bolig', 'over byggeri', 'over investering', 'over udlejning', 'overadministration', 'overinvestering', 'senior bolig', 'seniorbolig', 'udlejning', 'udlejnings', 'under Administration', 'under administration', 'under byggeri', 'underinvestering']\n",
            "  - titles: ['Bygherrerådgiver', 'Developer', 'Developers', 'Ejendomsadministrator', 'Ejendomsmægler', 'Ejendomsmæglere', 'bygherrerådgiver', 'bygherrerådgivere', 'developer', 'developere', 'developers', 'ejendomsadministrator', 'ejendomsadministrators', 'ejendomsmægler', 'ejendomsmæglere', 'ejendomsmæglers']\n",
            "\n",
            "- finance:\n",
            "  - orgs: ['BANK', 'BANKE', 'Bank', 'BankAnalytiker', 'Bankanalytiker', 'Bankchef', 'Bankdirektør', 'Banke', 'Bankrådgiver', 'Banks', 'Børs', 'Børschef', 'Børsdirektør', 'Børse', 'Central Bank', 'Central bank', 'Centralbank', 'FORSIKRING', 'Forsikring', 'Forsikrings', 'Investering', 'Investerings', 'Kapitalfond', 'Kapitalfonde', 'Nationalbank', 'Nationalbankdirektør', 'Nationalbanks', 'PENSION', 'PENSIONS', 'PEnsion', 'Pension', 'Pensions', 'REALKREDIT', 'Realkredit', 'Realkredits', 'SENIORPENSION', 'Seniorpension', 'bank', 'bankanalytiker', 'bankchef', 'bankdirektør', 'banke', 'bankformand', 'bankleder', 'bankrådgiver', 'banks', 'børs', 'børsanalytiker', 'børschef', 'central bank', 'centralbank', 'forsikring', 'forsikrings', 'først pension', 'hovedbank', 'investering', 'investerings', 'kapitalfond', 'kapitalfonde', 'kapitalfonds', 'mellem Forsikring', 'mellem bank', 'mellem investering', 'mellem nationalbank', 'mellem pension', 'nationalbank', 'nationalbankchef', 'nationalbankdirektør', 'nationalbanks', 'over bank', 'over investering', 'over pension', 'overforsikring', 'overinvestering', 'pension', 'pensions', 'realkredit', 'seniorpension', 'specialbank', 'under Forsikring', 'underinvestering', 'vicenationalbank']\n",
            "  - keywords: ['AKTIE', 'AKTIER', 'Aktie', 'Aktieanalytiker', 'Aktiechef', 'Aktier', 'Akties', 'Børs', 'Børschef', 'Børsdirektør', 'Børse', 'FINANS', 'FUSION', 'Finans', 'Finansanalytiker', 'Finanschef', 'Finansdirektør', 'Fusion', 'Fusions', 'Investering', 'Investerings', 'MARKED', 'Marked', 'Markeds', 'Obligation', 'Obligatione', 'Obligations', 'Opkøb', 'RENTE', 'Rente', 'Renter', 'Rentes', 'Toprente', 'VALUTA', 'Valuta', 'aktie', 'aktieanalytiker', 'aktiechef', 'aktier', 'aktierådgiver', 'akties', 'aktiespecialist', 'børs', 'børsanalytiker', 'børschef', 'central marked', 'finans', 'finansanalytiker', 'finansansvarlig', 'finanschef', 'finansdirektør', 'finansrådgiver', 'fusion', 'fusione', 'fusions', 'først rente', 'gruppe aktie', 'hovedaktie', 'hovedmarked', 'investering', 'investerings', 'koncernfinans', 'marked', 'markede', 'markedkoordinator', 'markedleder', 'markeds', 'mellem Finans', 'mellem finans', 'mellem investering', 'mellem marked', 'mellem rente', 'obligation', 'obligatione', 'obligations', 'opkøb', 'opkøbe', 'opkøbs', 'over Finans', 'over aktie', 'over investering', 'over marked', 'over rente', 'overinvestering', 'overrente', 'rente', 'renteanalytiker', 'renter', 'rentes', 'under Finans', 'under finans', 'under opkøb', 'underinvestering', 'valuta', 'valutaanalytiker', 'valutachef', 'valutas']\n",
            "  - titles: ['Aktiestrateg', 'Analytiker', 'Analytikere', 'Bankdirektør', 'Centralbankdirektør', 'Chefanalytiker', 'Chefporteføljemanager', 'Cheføkonom', 'Finansdirektør', 'INVESTOR', 'Investor', 'Investore', 'Investors', 'Koncernfinansdirektør', 'Porteføljemanager', 'Senioranalytiker', 'Seniorøkonom', 'TopInvestor', 'Topinvestor', 'Topøkonom', 'aktiestrateg', 'analytiker', 'analytikere', 'analytikers', 'bankdirektør', 'bankdirektørs', 'centralbankdirektør', 'chef-økonom', 'chefanalytiker', 'chefporteføljemanager', 'cheføkonom', 'finansdirektør', 'gruppefinansdirektør', 'hovedinvestor', 'investor', 'investore', 'investors', 'junioranalytiker', 'koncernfinansdirektør', 'koncernøkonom', 'mellem investor', 'porteføljemanager', 'porteføljemanagere', 'porteføljemanagers', 'senior aktiestrateg', 'senior analytiker', 'senior valutahandler', 'senior økonom', 'senior-analytiker', 'senioranalytiker', 'seniorporteføljemanager', 'seniorøkonom', 'top-økonom', 'topøkonom', 'under investor', 'underbankdirektør', 'valutahandler', 'valutahandlere', 'vicebankdirektør', 'Økonom', 'økonom', 'økonoms']\n",
            "\n",
            "- industry:\n",
            "  - orgs: ['ERHVERV', 'Erhverv', 'Erhvervs', 'FABRIKS', 'Fabrik', 'Fabriks', 'INDUSTRI', 'Industri', 'Industrie', 'Industris', 'Koncern', 'Koncernchef', 'Koncerndirektør', 'Overproduktion', 'PRODUKTION', 'Produktion', 'Produktions', 'Senior Erhverv', 'SeniorErhverv', 'Seniorerhverv', 'Specialproduktion', 'VIRKSOMHED', 'VIrksomhed', 'Virksomhed', 'Virksomheds', 'central produktion', 'erhverv', 'erhverve', 'erhvervleder', 'erhvervs', 'fabrik', 'fabriks', 'først virksomhed', 'hovederhverv', 'hovedfabrik', 'hovedproduktion', 'hovedvirksomhed', 'industri', 'industriansvarlig', 'industrie', 'industris', 'koncern', 'koncernansvarlig', 'koncernchef', 'koncerndirektør', 'koncerne', 'koncerns', 'mellem erhverv', 'mellem fabrik', 'mellem industri', 'mellem produktion', 'mellem virksomhed', 'over erhverv', 'over industri', 'over produktion', 'overproduktion', 'produktion', 'produktions', 'specialfabrik', 'specialproduktion', 'specialvirksomhed', 'under Koncern', 'under produktion', 'underproduktion', 'virksomhed', 'virksomhede', 'virksomheds']\n",
            "  - keywords: ['ERHVERV', 'Erhverv', 'Erhvervs', 'FABRIKS', 'Fabrik', 'Fabriks', 'INDUSTRI', 'Industri', 'Industrie', 'Industris', 'Koncern', 'Koncernchef', 'Koncerndirektør', 'MARKED', 'Marked', 'Markeds', 'Overproduktion', 'PRODUKTION', 'Produktion', 'Produktions', 'Senior Erhverv', 'SeniorErhverv', 'Seniorerhverv', 'Specialproduktion', 'Supply Chain', 'Supply chain', 'VIRKSOMHED', 'VIrksomhed', 'Virksomhed', 'Virksomheds', 'central marked', 'central produktion', 'erhverv', 'erhverve', 'erhvervleder', 'erhvervs', 'fabrik', 'fabriks', 'først virksomhed', 'hovederhverv', 'hovedfabrik', 'hovedmarked', 'hovedproduktion', 'hovedvirksomhed', 'industri', 'industriansvarlig', 'industrie', 'industris', 'koncern', 'koncernansvarlig', 'koncernchef', 'koncerndirektør', 'koncerne', 'koncerns', 'marked', 'markede', 'markedkoordinator', 'markedleder', 'markeds', 'mellem erhverv', 'mellem fabrik', 'mellem industri', 'mellem marked', 'mellem produktion', 'mellem virksomhed', 'over erhverv', 'over industri', 'over marked', 'over produktion', 'overproduktion', 'produktion', 'produktions', 'specialfabrik', 'specialproduktion', 'specialvirksomhed', 'supply chain', 'under Koncern', 'under produktion', 'underproduktion', 'virksomhed', 'virksomhede', 'virksomheds']\n",
            "  - titles: ['Adm direktør', 'Administrerende Direktør', 'Administrerende direktør', 'Bestyrelsesformand', 'Fabriksdirektør', 'Hovedbestyrelsesformand', 'Koncernchef', 'Produktionschef', 'Vice-administrerende direktør', 'Viceadministrerende direktør', 'Vicebestyrelsesformand', 'Vicekoncernchef', 'adm direktør', 'administrerende direktør', 'administrerende direktøre', 'administrerende direktørs', 'bestyrelsesformand', 'bestyrelsesformands', 'fabriksdirektør', 'hovedbestyrelsesformand', 'koncernbestyrelsesformand', 'koncernchef', 'mellem administrerende direktør', 'mellem bestyrelsesformand', 'over administrerende direktør', 'produktionschef', 'senior produktionschef', 'under administrerende direktør', 'vice-administrerende direktør', 'viceadministrerende direktør', 'vicebestyrelsesformand', 'vicekoncernchef']\n",
            "\n",
            "- tech:\n",
            "  - orgs: ['AI', 'AIE', 'AIR', 'AIS', 'Ai', 'Air', 'Ais', 'DATA', 'DIGITAL', 'DIGITALE', 'DIgital', 'Data', 'Datas', 'Digital', 'Digitalanalytiker', 'Digitalchef', 'Digitale', 'Digitals', 'General Data', 'IT', 'ITE', 'ITR', 'ITS', 'ITs', 'It', 'Itr', 'Its', 'Koncern IT', 'Over It', 'Senior IT', 'Senior it', 'Senior-IT', 'Senior-It', 'SeniorDigital', 'Software', 'Softwaredirektør', 'Softwares', 'Special-software', 'StartUp', 'Startup', 'Startups', 'TECH', 'TECHs', 'TEKNOLOGI', 'Tech', 'Techs', 'Teknologi', 'Teknologichef', 'Teknologis', 'Under IT', 'ai', 'air', 'ais', 'central it', 'chef software', 'data', 'dataanalytiker', 'dataansvarlig', 'datachef', 'datakonsulent', 'datas', 'dataspecialist', 'digital', 'digitalchef', 'digitale', 'digitalrådgiver', 'digitals', 'først data', 'gruppe IT', 'gruppe it', 'it', 'its', 'koncern it', 'koncern-it', 'mellem IT', 'mellem Tech', 'mellem it', 'mellem software', 'mellem tech', 'mellem teknologi', 'over AI', 'over Ai', 'over IT', 'over It', 'over Software', 'over data', 'over digital', 'over it', 'over tech', 'over teknologi', 'senior-IT', 'senior-it', 'software', 'softwarechef', 'softwares', 'special software', 'specialteknologi', 'startup', 'startupkonsulent', 'startups', 'tech', 'techs', 'teknologi', 'teknologianalytiker', 'teknologichef', 'teknologidirektør', 'teknologikonsulent', 'teknologikoordinator', 'teknologirådgiver', 'teknologis', 'teknologispecialist', 'under IT', 'under it']\n",
            "  - keywords: ['Automatisering', 'DATA', 'DIGITAL', 'DIGITALE', 'DIgital', 'Data', 'Datas', 'Digital', 'Digitalanalytiker', 'Digitalchef', 'Digitale', 'Digitals', 'General Data', 'IT', 'ITE', 'ITR', 'ITS', 'ITs', 'Innovation', 'Innovations', 'It', 'Itr', 'Its', 'Koncern IT', 'Kunstig intelligens', 'Over It', 'Senior IT', 'Senior it', 'Senior-IT', 'Senior-It', 'SeniorDigital', 'Software', 'Softwaredirektør', 'Softwares', 'Special-software', 'TECH', 'TECHs', 'TEKNOLOGI', 'Tech', 'Techs', 'Teknologi', 'Teknologichef', 'Teknologis', 'Under IT', 'automatisering', 'automatiserings', 'central it', 'chef software', 'data', 'dataanalytiker', 'dataansvarlig', 'datachef', 'datakonsulent', 'datas', 'dataspecialist', 'digital', 'digitalchef', 'digitale', 'digitalrådgiver', 'digitals', 'først data', 'gruppe IT', 'gruppe it', 'innovation', 'innovations', 'it', 'its', 'koncern it', 'koncern-it', 'kunstig intelligens', 'mellem IT', 'mellem Tech', 'mellem innovation', 'mellem it', 'mellem software', 'mellem tech', 'mellem teknologi', 'over IT', 'over It', 'over Software', 'over data', 'over digital', 'over it', 'over kunstig intelligens', 'over tech', 'over teknologi', 'senior-IT', 'senior-it', 'software', 'softwarechef', 'softwares', 'special software', 'specialteknologi', 'tech', 'techs', 'teknologi', 'teknologianalytiker', 'teknologichef', 'teknologidirektør', 'teknologikonsulent', 'teknologikoordinator', 'teknologirådgiver', 'teknologis', 'teknologispecialist', 'under IT', 'under it']\n",
            "  - titles: ['Softwareudviklere', 'Udviklingschef', 'dataanalytiker', 'dataanalytikere', 'digital chef', 'softwareudvikler', 'softwareudviklere', 'udviklingschef']\n",
            "\n",
            "- regulatory:\n",
            "  - keywords: ['Beskatning', 'Centralforvaltning', 'Forvaltning', 'Forvaltnings', 'Først-politik', 'LOVGIVNING', 'Lovgivning', 'Myndighed', 'Myndigheds', 'POLITIK', 'PolitiK', 'Politik', 'Regulering', 'Seniorpolitik', 'Tilsyn', 'Tilsyns', 'beskatning', 'beskatnings', 'central lovgivning', 'central myndighed', 'centralforvaltning', 'forvaltning', 'forvaltninge', 'forvaltnings', 'først-politik', 'lovgivning', 'lovgivnings', 'mellem beskatning', 'mellem forvaltning', 'mellem lovgivning', 'mellem politik', 'mellem tilsyn', 'myndighed', 'myndighedr', 'myndigheds', 'over lovgivning', 'over politik', 'overbeskatning', 'overmyndighed', 'overregulering', 'politik', 'politike', 'politiks', 'regulering', 'reguleringr', 'regulerings', 'seniorforvaltning', 'seniorpolitik', 'specialregulering', 'tilsyn', 'tilsynchef', 'tilsyne', 'tilsyns', 'toppolitik', 'under Politik', 'under forvaltning', 'under regulering', 'under tilsyn', 'underregulering']\n",
            "  - titles: ['Afdelingschef', 'Afdelingschefs', 'Direktør', 'Direktørs', 'FORMAND', 'Formand', 'Formande', 'Formands', 'Generaldirektør', 'Gruppeformand', 'Hovedformand', 'Koncerndirektør', 'MINISTER', 'MInister', 'Mellem direktør', 'Minister', 'Ministere', 'Ministerpræsident', 'Ministers', 'Overdirektør', 'Seniordirektør', 'Seniorformand', 'Tilsynschef', 'Topdirektør', 'Under direktør', 'Underdirektør', 'Vicedirektør', 'Viceformand', 'Viceminister', 'afdelingschef', 'departementchef', 'direktør', 'direktøre', 'direktørs', 'forMAND', 'formand', 'formande', 'formands', 'først formand', 'generaldirektør', 'gruppeformand', 'hovedformand', 'juniorminister', 'koncerndirektør', 'mellem direktør', 'mellem formand', 'mellem minister', 'minister', 'ministere', 'ministerpræsident', 'ministerrådgiver', 'ministers', 'over Formand', 'over direktør', 'over formand', 'overdirektør', 'senior-formand', 'seniordirektør', 'seniorformand', 'styrelsesdirektør', 'tilsynschef', 'tilsynschefs', 'top minister', 'top-direktør', 'topdirektør', 'topminister', 'under formand', 'underdirektør', 'vice-formand', 'vice-minister', 'viceafdelingschef', 'vicedirektør', 'viceformand', 'viceminister']\n",
            "  - orgs: ['Centralforvaltning', 'DOMSTOL', 'Departement', 'Domstol', 'Domstole', 'Domstols', 'Forvaltning', 'Forvaltnings', 'Hovedstyrelse', 'Ministerium', 'Myndighed', 'Myndigheds', 'Styrelse', 'Styrelser', 'Styrelses', 'Tilsyn', 'Tilsyns', 'central myndighed', 'centralforvaltning', 'departement', 'departementchef', 'departements', 'domstol', 'domstole', 'domstols', 'forvaltning', 'forvaltninge', 'forvaltnings', 'hovedstyrelse', 'mellem departement', 'mellem forvaltning', 'mellem styrelse', 'mellem tilsyn', 'ministerium', 'ministeriums', 'myndighed', 'myndighedr', 'myndigheds', 'over departement', 'overmyndighed', 'seniorforvaltning', 'specialdomstol', 'styrelse', 'styrelser', 'styrelses', 'tilsyn', 'tilsynchef', 'tilsyne', 'tilsyns', 'topstyrelse', 'under forvaltning', 'under ministerium', 'under tilsyn', 'viceministerium']\n",
            "\n",
            "- education:\n",
            "  - orgs: ['Akademi', 'Akademichef', 'Akademie', 'Akademis', 'CentralSkole', 'Centralskole', 'FAKULTET', 'Fakultet', 'Fakultets', 'Gymnasium', 'Gymnasiums', 'Institut', 'Institutchef', 'Institutdirektør', 'Institute', 'Institutleder', 'Instituts', 'Mellemskole', 'OverSkole', 'SKOLE', 'SKOLER', 'SKole', 'Senior Akademi', 'Seniorakademi', 'Skole', 'Skolechef', 'Skoledirektør', 'Skoleformand', 'Skoleleder', 'Skoler', 'Skoles', 'Specialskole', 'Specialuddannelse', 'UDDANNELSE', 'UNiversitet', 'Uddannelse', 'Uddannelser', 'Uddannelses', 'Under uddannelse', 'Underskole', 'Universitet', 'Universitets', 'akademi', 'akademichef', 'akademidirektør', 'akademileder', 'akademis', 'central skole', 'centralskole', 'fakultet', 'fakultets', 'gymnasium', 'gymnasiums', 'hovedskole', 'hoveduddannelse', 'institut', 'institutchef', 'institutdirektør', 'institute', 'institutleder', 'instituts', 'mellem Institut', 'mellem Skole', 'mellem akademi', 'mellem skole', 'mellem uddannelse', 'mellem universitet', 'mellemskole', 'mellemuddannelse', 'over skole', 'over uddannelse', 'overuddannelse', 'skole', 'skoleansvarlig', 'skolechef', 'skoledirektør', 'skoleformand', 'skolekonsulent', 'skolekoordinator', 'skoleleder', 'skoler', 'skolerådgiver', 'skoles', 'special skole', 'special-skole', 'specialskole', 'specialuddannelse', 'top skole', 'topskole', 'topuniversitet', 'uddannelse', 'uddannelser', 'uddannelses', 'under Institut', 'under Universitet', 'under skole', 'under uddannelse', 'underskole', 'universitet', 'universitets', 'viceskole']\n",
            "  - keywords: ['ELEV', 'Elev', 'Elevformand', 'Forskning', 'Forsknings', 'Forskningspecialist', 'Læring', 'Lærings', 'Pædagogik', 'STUDENT', 'Specialpædagogik', 'Specialuddannelse', 'Specialundervisning', 'Student', 'Students', 'Topstudent', 'UDDANNELSE', 'UNDERVISNING', 'Uddannelse', 'Uddannelser', 'Uddannelses', 'Under uddannelse', 'Undervisning', 'Undervisnings', 'elev', 'elevansvarlig', 'eleve', 'elevformand', 'elevs', 'forskning', 'forskningleder', 'forsknings', 'gruppe undervisning', 'gruppeundervisning', 'hoveduddannelse', 'læring', 'lærings', 'mellem elev', 'mellem forskning', 'mellem læring', 'mellem pædagogik', 'mellem uddannelse', 'mellem undervisning', 'mellemuddannelse', 'over forskning', 'over uddannelse', 'over undervisning', 'overuddannelse', 'pædagogik', 'special-pædagogik', 'special-undervisning', 'specialelev', 'specialpædagogik', 'specialuddannelse', 'specialundervisning', 'student', 'students', 'topforskning', 'uddannelse', 'uddannelser', 'uddannelses', 'under forskning', 'under uddannelse', 'undervisning', 'undervisninge', 'undervisnings']\n",
            "  - titles: ['Dekan', 'Institutleder', 'Institutledere', 'Lektor', 'PROFESSOR', 'Professor', 'Professors', 'Rektor', 'Rektorformand', 'Rektors', 'Skoleleder', 'Skoleledere', 'Skolelederformand', 'Skoleleders', 'UNDERVISER', 'Uddannelseschef', 'Underviser', 'Undervisere', 'Vicerektor', 'Viceskoleleder', 'dekan', 'dekans', 'først lektor', 'hovedunderviser', 'institutleder', 'institutledere', 'institutleders', 'juniorprofessor', 'lektor', 'lektors', 'mellem professor', 'mellem rektor', 'mellem skoleleder', 'mellem underviser', 'over professor', 'over rektor', 'over skoleleder', 'professor', 'professore', 'professorr', 'professors', 'rektor', 'rektorformand', 'rektors', 'seniorlektor', 'skoleleder', 'skoleledere', 'skolelederformand', 'skoleleders', 'specialunderviser', 'topprofessor', 'topunderviser', 'uddannelseschef', 'under lektor', 'under professor', 'underviser', 'undervisere', 'undervisers', 'vice-skoleleder', 'viceinstitutleder', 'vicerektor', 'viceskoleleder']\n",
            "\n",
            "- healthcare:\n",
            "  - titles: ['Cheflæge', 'Chefsygeplejerske', 'Forskningschef', 'Generallæge', 'LÆGE', 'LÆGER', 'Læge', 'Lægeformand', 'Lægekonsulent', 'Læger', 'Læges', 'Overlæge', 'Overlæger', 'Oversygeplejerske', 'Produktchef', 'Regulatory Affairs', 'Seniorproduktchef', 'Specialist', 'Speciallæge', 'Sundhedsøkonom', 'Sygeplejerske', 'Sygeplejersker', 'cheflæge', 'chefsygeplejerske', 'forskningschef', 'først læge', 'først overlæge', 'generallæge', 'juniorproduktchef', 'læge', 'lægechef', 'lægeformand', 'lægekonsulent', 'læger', 'lægerådgiver', 'læges', 'mellem læge', 'mellem sygeplejerske', 'over læge', 'overlæge', 'overlæger', 'overlæges', 'oversygeplejerske', 'produktchef', 'regulatory affairs', 'senior produktchef', 'seniorspecialist', 'specialist', 'speciallæge', 'specialsygeplejerske', 'sundhedsøkonom', 'sundhedsøkonoms', 'sygeplejerske', 'sygeplejersker', 'sygeplejerskes', 'under overlæge']\n",
            "  - orgs: ['Apotek', 'Apoteks', 'BIOTEK', 'Biotek', 'Centralklinik', 'Cheflæge', 'General Hospital', 'Generallæge', 'Hospital', 'Hospitals', 'Klinik', 'Klinikansvarlig', 'Klinikchef', 'Klinikleder', 'LÆGE', 'LÆGER', 'Læge', 'Lægeformand', 'Lægekonsulent', 'Læger', 'Læges', 'Medicinal', 'Medicinalanalytiker', 'Medicinals', 'Overlæge', 'Regionhospital', 'SUNDHED', 'Speciallæge', 'Sundhed', 'Sundheds', 'Sundhedsstyrelse', 'Sundhedsstyrelser', 'Sundhedsstyrelses', 'apotek', 'apoteks', 'biotek', 'biotekanalytiker', 'biotekdirektør', 'bioteks', 'central klinik', 'centralhospital', 'cheflæge', 'først læge', 'først sundhed', 'generallæge', 'hospital', 'hospitale', 'hospitals', 'hovedapotek', 'hovedhospital', 'klinik', 'klinikansvarlig', 'klinikchef', 'klinikleder', 'kliniks', 'læge', 'lægechef', 'lægeformand', 'lægekonsulent', 'læger', 'lægerådgiver', 'læges', 'medicinal', 'medicinalanalytiker', 'medicinalchef', 'medicinaldirektør', 'medicinale', 'mellem Sundhed', 'mellem apotek', 'mellem biotek', 'mellem hospital', 'mellem læge', 'mellem medicinal', 'mellem sundhed', 'over læge', 'over sundhed', 'overlæge', 'specialhospital', 'specialklinik', 'speciallæge', 'sundhed', 'sundheddirektør', 'sundheds', 'sundhedspecialist', 'sundhedsstyrelse', 'sundhedsstyrelser', 'sundhedsstyrelses', 'under Sundhed']\n",
            "  - keywords: ['BEHANDLING', 'BIOTEK', 'Behandling', 'Behandlings', 'Biotek', 'DIABETES', 'Diabetes', 'Diabetesformand', 'Først behandling', 'Insulin', 'Insulins', 'Klinisk', 'Kliniske', 'MEDICIN', 'MEDICINE', 'Medicin', 'Medicine', 'Medicins', 'Patient', 'Patients', 'SUNDHED', 'SYGDOM', 'Sundhed', 'Sundheds', 'Sygdom', 'Sygdome', 'behandling', 'behandlings', 'biotek', 'biotekanalytiker', 'biotekdirektør', 'bioteks', 'diabetes', 'først behandling', 'først medicin', 'først sundhed', 'gruppebehandling', 'insulin', 'klinisk', 'kliniske', 'medicin', 'medicine', 'medicins', 'mellem Sundhed', 'mellem behandling', 'mellem biotek', 'mellem medicin', 'mellem patient', 'mellem sundhed', 'mellem sygdom', 'mellembehandling', 'over behandling', 'over medicin', 'over sundhed', 'over sygdom', 'over-behandling', 'overbehandling', 'patient', 'patientansvarlig', 'patiente', 'patients', 'special behandling', 'specialbehandling', 'sundhed', 'sundheddirektør', 'sundheds', 'sundhedspecialist', 'sygdom', 'sygdome', 'sygdoms', 'under Sundhed', 'under behandling', 'under sygdom', 'underbehandling']\n",
            "\n",
            "- politics:\n",
            "  - keywords: ['Beslutning', 'Beslutnings', 'DEMOKRATI', 'Demokrati', 'Demokratis', 'Demokratispecialist', 'Først-politik', 'LOVGIVNING', 'Lovgivning', 'POLITIK', 'PolitiK', 'Politik', 'REFORM', 'Reform', 'Reforms', 'Samfund', 'Samfunde', 'Samfunds', 'Seniorpolitik', 'Under valg', 'VALG', 'Valg', 'Valganalytiker', 'Valgansvarlig', 'Valgformand', 'Valgkonsulent', 'Valgs', 'beslutning', 'beslutninge', 'beslutnings', 'central beslutning', 'central lovgivning', 'demokrati', 'demokratie', 'demokratileder', 'demokratirådgiver', 'demokratis', 'først valg', 'først-politik', 'førstvalg', 'general beslutning', 'gruppebeslutning', 'lovgivning', 'lovgivnings', 'mellem demokrati', 'mellem lovgivning', 'mellem politik', 'mellem samfund', 'mellem valg', 'over beslutning', 'over demokrati', 'over lovgivning', 'over politik', 'over valg', 'politik', 'politike', 'politiks', 'reform', 'reformpræsident', 'reforms', 'samfund', 'samfunde', 'samfunds', 'seniorpolitik', 'toppolitik', 'under Politik', 'under reform', 'under valg', 'valg', 'valganalytiker', 'valgansvarlig', 'valgchef', 'valge', 'valgformand', 'valgkonsulent', 'valgrådgiver', 'valgs']\n",
            "  - orgs: ['BYRÅD', 'Byråd', 'Byråds', 'FOLKETING', 'FOLKETINGS', 'Folketing', 'Folketinge', 'Folketings', 'Generalkommission', 'Kommission', 'Kommissions', 'Kommunalbestyrelse', 'Kommunalbestyrelser', 'Kommunalbestyrelses', 'Ministerium', 'PARTI', 'Parti', 'Partichef', 'Partie', 'Partiformand', 'Partileder', 'Partis', 'REGERING', 'Regering', 'Regerings', 'byråd', 'byråds', 'central regering', 'centralregering', 'folketing', 'folketinge', 'folketings', 'hovedkommission', 'hovedparti', 'juniorparti', 'kommission', 'kommissionformand', 'kommissions', 'kommunalbestyrelse', 'kommunalbestyrelser', 'kommunalbestyrelses', 'mellem Folketing', 'mellem byråd', 'mellem folketing', 'mellem parti', 'mellem regering', 'ministerium', 'ministeriums', 'over kommission', 'overparti', 'parti', 'partichef', 'partie', 'partiformand', 'partikonsulent', 'partileder', 'partirådgiver', 'partis', 'regering', 'regeringchef', 'regeringe', 'regerings', 'under byråd', 'under ministerium', 'under regering', 'viceministerium']\n",
            "  - titles: ['Borgmester', 'Borgmesters', 'Folketingsmedlem', 'Kommunalbestyrelsesmedlem', 'MFer', 'MFere', 'MINISTER', 'MInister', 'Minister', 'Ministere', 'Ministerpræsident', 'Ministers', 'Overborgmester', 'POLITIKER', 'POLITIKERE', 'Politiker', 'Politikere', 'Politikerleder', 'REGIONSRÅDSFORMAND', 'Regionsrådsformand', 'Rådmand', 'Rådmands', 'Toppolitiker', 'Under borgmester', 'Viceborgmester', 'Viceminister', 'borgmester', 'borgmestere', 'borgmesters', 'central politiker', 'folketingsmedlem', 'folketingsmedlems', 'juniorminister', 'kommunalbestyrelsesmedlem', 'kommunalbestyrelsesmedlems', 'mellem borgmester', 'mellem folketingsmedlem', 'mellem minister', 'mellem politiker', 'mellem rådmand', 'minister', 'ministere', 'ministerpræsident', 'ministerrådgiver', 'ministers', 'over borgmester', 'over folketingsmedlem', 'overborgmester', 'politikEr', 'politiker', 'politikere', 'politikers', 'regionsrådsformand', 'regionsrådsformande', 'rådmand', 'rådmands', 'top minister', 'top-politiker', 'topminister', 'toppolitiker', 'under borgmester', 'under rådmand', 'vice-borgmester', 'vice-minister', 'viceborgmester', 'viceminister']\n",
            "\n",
            "- aviation:\n",
            "  - keywords: ['Airline', 'Airlines', 'Aviation', 'FLY', 'Fly', 'Flye', 'Flyr', 'Flys', 'Koncern Sikkerhed', 'LUFTHAVN', 'Luftfart', 'Lufthavn', 'Lufthavne', 'Lufthavns', 'Over transport', 'REJSE', 'REJSER', 'Rejse', 'Rejseleder', 'Rejser', 'Rejses', 'SIKKERHED', 'Sikkerhed', 'Sikkerheds', 'Specialtransport', 'TRANSPORT', 'Transport', 'Transporte', 'Transports', 'Under rejse', 'airline', 'airlines', 'aviation', 'fly', 'flye', 'flyleder', 'flyr', 'flys', 'først fly', 'først rejse', 'grupperejse', 'hovedfly', 'hovedlufthavn', 'hovedtransport', 'luftfart', 'luftfarts', 'lufthavn', 'lufthavne', 'lufthavns', 'mellem fly', 'mellem luftfart', 'mellem lufthavn', 'mellem sikkerhed', 'over lufthavn', 'over rejse', 'over sikkerhed', 'over transport', 'rejse', 'rejsedirektør', 'rejsekonsulent', 'rejseleder', 'rejser', 'rejses', 'sikkerhed', 'sikkerhedchef', 'sikkerhede', 'sikkerheds', 'specialfly', 'transport', 'transportansvarlig', 'transportchef', 'transportdirektør', 'transporte', 'transportkoordinator', 'transportrådgiver', 'under Transport', 'under rejse', 'under transport']\n",
            "  - orgs: ['Airline', 'Airlines', 'Aviation', 'Flyproducent', 'Flyselskab', 'LUFTHAVN', 'Luftfart', 'Lufthavn', 'Lufthavne', 'Lufthavns', 'airline', 'airlines', 'aviation', 'flyproducent', 'flyselskab', 'flyselskabs', 'hovedlufthavn', 'luftfart', 'luftfarts', 'lufthavn', 'lufthavne', 'lufthavns', 'mellem luftfart', 'mellem lufthavn', 'over lufthavn']\n",
            "  - titles: ['Flyveleder', 'Flyveledere', 'Kabinechef', 'Lufthavnsdirektør', 'PILOT', 'Pilot', 'Pilote', 'Pilotformand', 'Pilots', 'Teknisk Chef', 'Teknisk chef', 'chefpilot', 'flyveleder', 'flyveledere', 'flyveleders', 'lufthavnsdirektør', 'pilot', 'pilotformand', 'pilots', 'teknisk chef', 'under teknisk chef']\n",
            "\n",
            "- design:\n",
            "  - keywords: ['BRUGER', 'Bruger', 'Brugere', 'DESIGN', 'Design', 'Designchef', 'Designdirektør', 'Designe', 'Designs', 'Først bruger', 'Junior Design', 'KUNST', 'Kreativ', 'Kreative', 'Kunst', 'Kunstkonsulent', 'Kunsts', 'MODE', 'MODER', 'Mode', 'Moder', 'Modes', 'Produkt', 'Produktchef', 'Produktrådgiver', 'Produktspecialist', 'Special design', 'Under Kunst', 'bruger', 'brugere', 'brugers', 'design', 'designDirektør', 'designchef', 'designdirektør', 'designe', 'designs', 'først bruger', 'først design', 'gruppe bruger', 'hovedprodukt', 'koncern bruger', 'kreativ', 'kreative', 'kunst', 'kunstansvarlig', 'kunstchef', 'kunstdirektør', 'kunste', 'kunstformand', 'kunstkonsulent', 'kunstleder', 'kunstrådgiver', 'kunsts', 'mellem Design', 'mellem bruger', 'mellem design', 'mellem kunst', 'mellem mode', 'mellem produkt', 'mellem æstetik', 'mode', 'modedirektør', 'modekonsulent', 'moder', 'modes', 'over bruger', 'over design', 'over kunst', 'over mode', 'over æstetik', 'over-produkt', 'overbruger', 'produkt', 'produktansvarlig', 'produktchef', 'produktdirektør', 'produktkoordinator', 'produkts', 'produktspecialist', 'special design', 'specialprodukt', 'top bruger', 'top-design', 'topdesign', 'topprodukt', 'under Kunst', 'under bruger', 'under design', 'Æstetik', 'æstetik', 'æstetiks']\n",
            "  - orgs: ['Kreativ', 'Kreative', 'MODE', 'MODER', 'Mode', 'Modehuse', 'Moder', 'Modes', 'Tegnestue', 'Tegnestuer', 'designfirma', 'designstudie', 'designstudier', 'kreativ', 'kreative', 'mellem mode', 'mode', 'modedirektør', 'modehus', 'modehuse', 'modekonsulent', 'moder', 'modes', 'over mode', 'tegnestue', 'tegnestuechef', 'tegnestuer', 'tegnestues']\n",
            "  - titles: ['Art Director', 'Chefdesigner', 'DESIGNER', 'DESIGNERS', 'Designchef', 'Designer', 'Designere', 'Designers', 'Kreativ direktør', 'Modeekspert', 'Senior Designer', 'Topdesigner', 'art director', 'chefdesigner', 'designchef', 'designer', 'designere', 'designers', 'hoveddesigner', 'kreativ direktør', 'mellem designer', 'modeekspert', 'produktudvikler', 'produktudviklere', 'senior art director', 'seniordesigner', 'specialdesigner', 'topdesigner']\n",
            "\n",
            "- architecture:\n",
            "  - keywords: ['ARKITEKTUR', 'Arkitektur', 'BYGGERI', 'Byggeri', 'Byggeris', 'Byplanlægning', 'Byrum', 'Byrums', 'DESIGN', 'Design', 'Designchef', 'Designdirektør', 'Designe', 'Designs', 'Junior Design', 'Landskab', 'Landskabs', 'Special design', 'arkitektur', 'arkitekturs', 'byggeri', 'byggerie', 'byggeris', 'byplanlægning', 'byplanlægnings', 'byrum', 'byrums', 'design', 'designDirektør', 'designchef', 'designdirektør', 'designe', 'designs', 'først design', 'landskab', 'landskabe', 'landskabs', 'mellem Design', 'mellem arkitektur', 'mellem design', 'mellem landskab', 'mellem æstetik', 'over arkitektur', 'over byggeri', 'over byplanlægning', 'over design', 'over æstetik', 'special design', 'top-design', 'topdesign', 'under byggeri', 'under design', 'Æstetik', 'æstetik', 'æstetiks']\n",
            "  - orgs: ['ARKITEKTUR', 'Arkitektfirma', 'Arkitektur', 'Byplanlægning', 'DESIGN', 'Design', 'Designchef', 'Designdirektør', 'Designe', 'Designs', 'Junior Design', 'Special design', 'Tegnestue', 'Tegnestuer', 'arkitektfirma', 'arkitektfirmas', 'arkitektur', 'arkitekturs', 'byplanlægning', 'byplanlægnings', 'design', 'designDirektør', 'designchef', 'designdirektør', 'designe', 'designs', 'først design', 'mellem Design', 'mellem arkitektur', 'mellem design', 'over arkitektur', 'over byplanlægning', 'over design', 'special design', 'tegnestue', 'tegnestuechef', 'tegnestuer', 'tegnestues', 'top-design', 'topdesign', 'under design']\n",
            "  - titles: ['ARKITEKT', 'Arkitekt', 'Arkitekts', 'Bygningsdesignere', 'Byplanlægger', 'Byplanlæggere', 'Chefarkitekt', 'Hovedarkitekt', 'Kreativ direktør', 'Landskabsarkitekt', 'Overarkitekt', 'PARTNER', 'Partner', 'Partnere', 'Partners', 'Senior Partner', 'Seniorpartner', 'arkitekt', 'arkitekts', 'byplanlægger', 'byplanlæggere', 'chefarkitekt', 'hovedarkitekt', 'hovedpartner', 'juniorpartner', 'kreativ direktør', 'landskabsarkitekt', 'mellem arkitekt', 'over arkitekt', 'over partner', 'overarkitekt', 'partner', 'partnerchef', 'partnerdirektør', 'partnere', 'partnerr', 'partners', 'senior partner', 'seniorpartner']\n",
            "\n",
            "- hospitality:\n",
            "  - keywords: ['Central Hotel', 'Chef-Mad', 'Gastronomi', 'HOTEL', 'HOTELS', 'Hotel', 'Hoteldirektør', 'Hotels', 'Koncernservice', 'MAD', 'MADE', 'MADS', 'MADs', 'MAds', 'Mad', 'Made', 'Mads', 'OPLEVELSE', 'Oplevelse', 'Oplevelser', 'Oplevelses', 'Over Hotel', 'Overnatning', 'RESTAURANT', 'Restaurant', 'Restaurantchef', 'Restaurante', 'Restaurants', 'SERVICE', 'Senior Service', 'Senior-service', 'Service', 'Servicechef', 'Serviceleder', 'Servicer', 'Servicerådgiver', 'Services', 'først mad', 'gastronomi', 'gastronomis', 'general service', 'hotel', 'hotelchef', 'hoteldirektør', 'hotels', 'hovedrestaurant', 'koncern service', 'koncernservice', 'mad', 'made', 'madr', 'mads', 'madspecialist', 'mellem Hotel', 'mellem Restaurant', 'mellem gastronomi', 'mellem hotel', 'mellem mad', 'mellem restaurant', 'mellem service', 'mellemmad', 'oplevelse', 'oplevelser', 'oplevelses', 'over Hotel', 'over gastronomi', 'over hotel', 'over mad', 'over oplevelse', 'over restaurant', 'over service', 'overnatning', 'overnatnings', 'restaurant', 'restaurantchef', 'restaurante', 'restaurantleder', 'restaurants', 'seniorservice', 'service', 'servicechef', 'servicedirektør', 'servicekoordinator', 'serviceleder', 'servicer', 'servicerådgiver', 'services', 'servicespecialist', 'specialrestaurant', 'top-restaurant', 'topgastronomi', 'topoplevelse', 'toprestaurant', 'topservice', 'under Hotel', 'under overnatning', 'under service']\n",
            "  - orgs: ['Catering', 'Caterings', 'Central Hotel', 'Gastronomi', 'HOTEL', 'HOTELS', 'Hospitality', 'Hotel', 'Hoteldirektør', 'Hotels', 'Over Hotel', 'RESTAURANT', 'Restaurant', 'Restaurantchef', 'Restaurante', 'Restaurants', 'catering', 'caterings', 'gastronomi', 'gastronomis', 'hospitality', 'hotel', 'hotelchef', 'hoteldirektør', 'hotels', 'hovedrestaurant', 'mellem Hotel', 'mellem Restaurant', 'mellem gastronomi', 'mellem hotel', 'mellem restaurant', 'over Hotel', 'over gastronomi', 'over hotel', 'over restaurant', 'restaurant', 'restaurantchef', 'restaurante', 'restaurantleder', 'restaurants', 'specialrestaurant', 'top-restaurant', 'topgastronomi', 'toprestaurant', 'under Hotel']\n",
            "  - titles: ['Chefkok', 'Chefsommelier', 'KOK', 'KOKS', 'KOk', 'Kok', 'Koke', 'Koks', 'Køkkenchef', 'Overkok', 'Restaurantchef', 'Restauratør', 'Sommelier', 'Sommeliers', 'Underkok', 'chefkok', 'chefsommelier', 'hotelchef', 'kok', 'koks', 'køkkenchef', 'køkkenchefs', 'mellem kok', 'mellem køkkenchef', 'mellem restauratør', 'overkok', 'restaurantchef', 'restauratør', 'restauratørformand', 'restauratørs', 'sommelier', 'sommeliers', 'topkok', 'underkok']\n",
            "\n",
            "- tourism:\n",
            "  - orgs: ['Destination', 'Destinations', 'Overturisme', 'REJSE', 'REJSER', 'Rejse', 'Rejsebureau', 'Rejsebureaus', 'Rejseleder', 'Rejser', 'Rejses', 'TURISME', 'Turisme', 'Turismechef', 'Turismekonsulent', 'Turismes', 'Turistkontor', 'Under rejse', 'destination', 'destinations', 'først rejse', 'grupperejse', 'mellem Destination', 'mellem turisme', 'over rejse', 'rejse', 'rejsebureau', 'rejsebureaur', 'rejsebureaus', 'rejsedirektør', 'rejsekonsulent', 'rejseleder', 'rejser', 'rejses', 'turisme', 'turismechef', 'turismedirektør', 'turismekonsulent', 'turismes', 'turistkontor', 'under rejse']\n",
            "  - keywords: ['Attraktion', 'Destination', 'Destinations', 'FERIE', 'Ferie', 'Ferier', 'OPLEVELSE', 'Oplevelse', 'Oplevelser', 'Oplevelses', 'Overturisme', 'REJSE', 'REJSER', 'Rejse', 'Rejseleder', 'Rejser', 'Rejses', 'TURISME', 'TURIST', 'Turisme', 'Turismechef', 'Turismekonsulent', 'Turismes', 'Turist', 'Turistchef', 'Turistdirektør', 'Turistkonsulent', 'Turists', 'Under ferie', 'Under rejse', 'attraktion', 'destination', 'destinations', 'ferie', 'feriekonsulent', 'ferier', 'ferierådgiver', 'først ferie', 'først rejse', 'grupperejse', 'hovedattraktion', 'hovedferie', 'mellem Destination', 'mellem ferie', 'mellem turisme', 'oplevelse', 'oplevelser', 'oplevelses', 'over ferie', 'over oplevelse', 'over rejse', 'rejse', 'rejsedirektør', 'rejsekonsulent', 'rejseleder', 'rejser', 'rejses', 'topattraktion', 'topoplevelse', 'turisme', 'turismechef', 'turismedirektør', 'turismekonsulent', 'turismes', 'turist', 'turistchef', 'turistdirektør', 'turiste', 'turistformand', 'turistkonsulent', 'turistkoordinator', 'turists', 'under ferie', 'under rejse']\n",
            "  - titles: ['Rejseleder', 'Turistchef', 'destinationschef', 'rejseleder', 'rejseledere', 'turistchef']\n",
            "\n",
            "- appliances:\n",
            "  - orgs: ['Chefforhandler', 'Distribution', 'Distributions', 'Elektronik', 'FORHANDLER', 'Forhandler', 'Forhandlere', 'Hvidevare', 'Hvidevarer', 'Producent', 'Topforhandler', 'chef-forhandler', 'chefforhandler', 'chefproducent', 'distribution', 'distributions', 'elektronik', 'forhandler', 'forhandlere', 'forhandlers', 'gruppe forhandler', 'hovedforhandler', 'hovedproducent', 'hvidevare', 'hvidevarer', 'koncern forhandler', 'mellem elektronik', 'mellem producent', 'over distribution', 'over elektronik', 'producent', 'producentchef', 'producentformand', 'producents', 'seniorforhandler', 'top-producent', 'topforhandler']\n",
            "  - keywords: ['Elektronik', 'Hvidevare', 'Hvidevarer', 'Innovation', 'Innovations', 'Koncernudvikling', 'KØKKEN', 'Køkken', 'Køkkenchef', 'Køkkenleder', 'Køkkens', 'Produkt', 'Produktchef', 'Produktrådgiver', 'Produktspecialist', 'TEKNISKE', 'Teknisk', 'Tekniske', 'UDVIKLING', 'Udvikling', 'Udviklings', 'Under udvikling', 'central køkken', 'centralkøkken', 'elektronik', 'hovedprodukt', 'hovedudvikling', 'hvidevare', 'hvidevarer', 'innovation', 'innovations', 'koncernudvikling', 'køkken', 'køkkenansvarlig', 'køkkenchef', 'køkkene', 'køkkenleder', 'køkkens', 'køkkenspecialist', 'mellem Teknisk', 'mellem elektronik', 'mellem innovation', 'mellem køkken', 'mellem produkt', 'mellem teknisk', 'mellem udvikling', 'over elektronik', 'over køkken', 'over udvikling', 'over-produkt', 'produkt', 'produktansvarlig', 'produktchef', 'produktdirektør', 'produktkoordinator', 'produkts', 'produktspecialist', 'specialprodukt', 'specialudvikling', 'teknisk', 'tekniskchef', 'tekniske', 'topprodukt', 'udvikling', 'udviklingchef', 'udviklinge', 'udviklings', 'under køkken', 'under teknisk', 'under udvikling', 'underudvikling']\n",
            "  - titles: ['Chefingeniør', 'INGENIØR', 'Ingeniør', 'Ingeniørrådgiver', 'Overingeniør', 'Produktchef', 'Salgschef', 'Seniorproduktchef', 'Teknisk Chef', 'Teknisk chef', 'Udviklingschef', 'chefingeniør', 'ingeniør', 'ingeniøre', 'ingeniørs', 'juniorproduktchef', 'overingeniør', 'produkt Direktør', 'produktchef', 'salgschef', 'senior produktchef', 'senior salgschef', 'senioringeniør', 'seniorsalgschef', 'teknisk chef', 'udviklingschef', 'under teknisk chef']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}